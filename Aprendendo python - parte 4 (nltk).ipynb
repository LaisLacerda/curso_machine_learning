{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Nltk</h1>\n",
    "\n",
    "<a href=\"https://www.nltk.org/book/\">Referência sobre Nltk</a>\n",
    "\n",
    "O Nltk é uma biblioteca para processamento de texto natural em python. Com ela é possível realizar classificação de textos, treinar chunkers, entre outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, uma função bastante útil para o processamento de textos em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def normalize_text(text, to_lower=True):\n",
    "    text = normalize('NFKD', text).encode('ASCII', 'ignore')\n",
    "\n",
    "    final_text = text\n",
    "\n",
    "    if to_lower:\n",
    "        final_text = text.lower()\n",
    "        \n",
    "    return final_text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola, tudo bem? como voce vai?\n",
      "Ola, tudo bem? como voce vai?\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text('OlÁ, tudo bem? como você vai?'))\n",
    "print(normalize_text('Olá, tudo bem? como você vai?', to_lower=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao realizar a normalização de textos, considere excluir casos como o verbo de ligação \"é\" e o símbolo de parágrafo &para;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baixando corpora e pacotes úteis</h2\n",
    "<br><br>\n",
    "Vamos agora baixar exemplos de textos em português para trabalhar e alguns pacotes que serão úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Baixar: book\n",
    "#Baixar: all-corpora\n",
    "#Baixar: floresta\n",
    "#Baixar: machado\n",
    "#Baixar: punkt\n",
    "#Baixar: stopwords\n",
    "\n",
    "#Se houverem problemas com a interface. Abrir o terminal do Jupyter e inserir os comandos:\n",
    "#python -m nltk downloader book\n",
    "#python -m nltk downloader all-corpora\n",
    "#python -m nltk.downloader floresta\n",
    "#python -m nltk.downloader machado\n",
    "#python -m nltk.downloader punkt\n",
    "#python -m nltk.downloader stopwords\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Carregando textos e conhecendo funções úteis do Nltk</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segmentação de texto</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenização</h3>\n",
    "<br>\n",
    "Tokenização é o processo de dividir uma frase em palavras particulares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meu', 'nome', 'é', 'felipe', 'navarro']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('meu nome é felipe navarro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Segmentação em sentenças (sent tokenization)</h3>\n",
    "<br>\n",
    "É o processo de quebrar um texto grande em frases individuais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meu nome é João da Silva e gosto de passear no feriado.',\n",
       " 'Recentemente comprei um carro, que quebrou ontem']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "\n",
    "texto = 'Meu nome é João da Silva e gosto de passear no feriado. Recentemente comprei um carro, que quebrou ontem'\n",
    "\n",
    "sent_tokenizer.tokenize(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para simplificar vamos criar funções\n",
    "\n",
    "def tokens(my_str):\n",
    "    return nltk.word_tokenize(my_str)\n",
    "\n",
    "def sent_seg(my_str):\n",
    "    return sent_tokenizer.tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classificação sintática</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualizando corpus taggeado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', 'é', 'um', 'ex-libris', 'de', 'a']\n",
      "----------\n",
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj'), ('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import floresta\n",
    "#Visualizando as palavras\n",
    "print(floresta.words()[:10])\n",
    "\n",
    "print('----------')\n",
    "#Com tags\n",
    "print(floresta.tagged_words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'art'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_tag(tag):\n",
    "    try:\n",
    "        return tag.split('+')[1]\n",
    "    except:\n",
    "        return tag\n",
    "\n",
    "simple_tag('>N+art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj')]\n",
      "--------\n",
      "[('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), ('algarvia', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('uma', 'H+num'), ('de', 'H+prp'), ('as', '>N+art'), ('mais', '>A+adv'), ('antigas', 'H+adj'), ('discotecas', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Algarve', 'H+prop'), (',', ','), ('situada', 'P+v-pcp'), ('em', 'H+prp'), ('Albufeira', 'P<+prop'), (',', ','), ('que', 'SUBJ+pron-indp'), ('continua', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('manter', 'MV+v-inf'), ('os', '>N+art'), ('traços', 'H+n'), ('decorativos', 'N<+adj'), ('e', 'CO+conj-c'), ('as', '>N+art'), ('clientelas', 'H+n'), ('de', 'H+prp'), ('sempre', 'P<+adv'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('um_pouco', 'ADVL+adv'), ('a', '>N+art'), ('versão', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('espécie', 'H+n'), ('de', 'H+prp'), ('«', '«'), ('outro', '>N+pron-det'), ('lado', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), (',', ','), ('a', 'H+prp'), ('meio', '>N+adj'), ('caminho', 'H+n'), ('entre', 'H+prp'), ('os', '>N+art'), ('devaneios', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('fauna', 'H+n'), ('periférica', 'N<+adj'), (',', ','), ('seja', 'P+v-fin'), ('de', 'H+prp'), ('Lisboa', 'CJT+prop'), (',', ','), ('Londres', 'CJT+prop'), (',', ','), ('Dublin', 'CJT+prop'), ('ou', 'CO+conj-c'), ('Faro', 'CJT+n'), ('e', 'CO+conj-c'), ('Portimão', 'CJT+prop'), (',', ','), ('e', 'CO+conj-c'), ('a', '>N+art'), ('postura', 'H+n'), ('circunspecta', 'N<+adj'), ('de', 'H+prp'), ('os', '>N+art'), ('fiéis', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('casa', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('de', 'H+prp'), ('ela', 'P<+pron-pers'), ('esperam', 'P+v-fin'), ('a', '>N+art'), ('música', 'H+n'), ('«', '«'), ('geracionista', 'N<+n'), ('de', 'H+prp'), ('os', '>A+art'), ('60', 'H+num'), ('ou', 'CO+conj-c'), ('de', 'H+prp'), ('os', '>A+art'), ('70', 'H+num'), ('.', '.')]\n",
      "--------\n",
      "[('Não', 'ADVL+adv'), ('deixa', 'AUX+v-fin'), ('de', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('tempos', 'H+n'), ('que', 'SUBJ+pron-indp'), ('correm', 'P+v-fin'), (',', ','), ('um', '>N+art'), ('certo', '>N+pron-det'), ('«', '«'), ('very_typical', 'H+n'), ('algarvio', 'N<+adj'), (',', ','), ('cabeça', 'H+n'), ('de', 'H+prp'), ('cartaz', 'P<+n'), ('para', 'H+prp'), ('os', 'H+pron-det'), ('que', 'SUBJ+pron-indp'), ('querem', 'P+v-fin'), ('fugir', 'P+v-inf'), ('a', 'H+prp'), ('algumas', '>N+pron-det'), ('movimentações', 'H+n'), ('nocturnas', 'N<+adj'), ('já', '>A+adv'), ('a', 'H+prp'), ('caminho', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('ritualização', 'H+n'), ('de', 'H+prp'), ('massas', 'P<+n'), (',', ','), ('de', 'H+prp'), ('o', '>N+art'), ('género', 'H+n'), ('«', '«'), ('vamos', 'P+v-fin'), ('todos', 'SUBJ+pron-det'), ('a', 'H+prp'), ('o', '>N+art'), ('Calypso', 'H+prop'), ('e', 'CO+conj-c'), ('encontramos-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('em', 'H+prp'), ('a', '>N+art'), ('Locomia', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('assim', 'ADVL+adv'), (',', ','), ('a', 'H+prp'), ('os', '>N+art'), ('2,5', '>N+num'), ('milhões', 'H+n'), ('que', 'ACC+pron-indp'), ('o', '>N+art'), ('Ministério_do_Planeamento_e_Administração_do_Território', 'H+prop'), ('já', 'ADVL+adv'), ('gasta', 'P+v-fin'), ('em', 'H+prp'), ('o', '>N+art'), ('pagamento', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('pessoal', 'H+n'), ('afecto', 'P+v-pcp'), ('a', 'H+prp'), ('estes', '>N+pron-det'), ('organismos', 'H+n'), (',', ','), ('vêm', 'P+v-fin'), ('juntar-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('os', '>N+art'), ('montantes', 'H+n'), ('de', 'H+prp'), ('as', '>N+art'), ('obras', 'H+n'), ('propriamente', '>A+adv'), ('ditas', 'H+adj'), (',', ','), ('que', 'ACC+pron-indp'), ('os', '>N+art'), ('municípios', 'H+n'), (',', ','), ('já', '>A+adv'), ('com', 'H+prp'), ('projectos', 'H+n'), ('em', 'H+prp'), ('a', '>N+art'), ('mão', 'H+n'), (',', ','), ('vêm', 'AUX+v-fin'), ('reivindicar', 'MV+v-inf'), ('junto', 'H+adv'), ('de', 'H+prp'), ('o', '>N+art'), ('Executivo', 'H+n'), (',', ','), ('como', 'H+prp'), ('salienta', 'P+v-fin'), ('aquele', '>N+pron-det'), ('membro', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Governo', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('o', '>N+art'), ('dinheiro', 'H+n'), ('«', '«'), ('não', 'ADVL+adv'), ('falta', 'P+v-fin'), ('só', '>A+adv'), ('a', 'H+prp'), ('as', '>N+art'), ('câmaras', 'H+n'), (',', ','), ('lembra', 'P+v-fin'), ('o', '>N+art'), ('secretário_de_Estado', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('considera', 'P+v-fin'), ('que', 'SUB+conj-s'), ('a', '>N+art'), ('solução', 'H+n'), ('para', 'H+prp'), ('as', '>N+art'), ('autarquias', 'H+n'), ('é', 'P+v-fin'), ('«', '«'), ('especializarem-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('em', 'H+prp'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('Mas', 'CO+conj-c'), ('como', 'ADVL+adv'), (',', ','), ('se', 'SUB+conj-s'), ('muitas', 'SUBJ+pron-det'), ('não', 'ADVL+adv'), ('dispõem', 'P+v-fin'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('seus', '>N+pron-det'), ('quadros', 'H+n'), (',', ','), ('de', 'H+prp'), ('os', '>N+art'), ('técnicos', 'H+n'), ('necessários', 'N<+adj'), ('?', '?')]\n",
      "--------\n",
      "[('«', '«'), ('Encomendem-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('a', 'H+prp'), ('projectistas', 'H+n'), ('de_fora', 'N<+pp'), ('porque', 'SUB+conj-s'), (',', ','), ('se', 'SUB+conj-s'), ('as', '>N+art'), ('obras', 'H+n'), ('vierem', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('ser', 'AUX+v-inf'), ('financiadas', 'MV+v-pcp'), (',', ','), ('eles', 'SUBJ+pron-pers'), ('até', 'ADVL+adv'), ('saem', 'P+v-fin'), ('de_graça', 'ADVS+pp'), (',', ','), ('já_que', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('esse', '>N+pron-det'), ('caso', 'H+n'), (',', ','), ('«', '«'), ('os', '>N+art'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('pagam', 'P+v-fin'), ('os', '>N+art'), ('projectos', 'H+n'), (',', ','), ('o', '>N+art'), ('mesmo', 'H+pron-det'), ('não', 'ADVL+adv'), ('acontecendo', 'P+v-ger'), ('quando', 'ADVL+adv'), ('eles', 'SUBJ+pron-pers'), ('são', 'AUX+v-fin'), ('feitos', 'MV+v-pcp'), ('por', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), (',', ','), ('dado', 'P+v-pcp'), ('serem', 'P+v-inf'), ('organismos', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Estado', 'H+n'), ('.', '.')]\n",
      "--------\n",
      "[('Essa', 'SUBJ+pron-det'), ('poderá', 'AUX+v-fin'), ('vir', 'AUX+v-inf'), ('a', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), ('uma', '>N+art'), ('hipótese', 'H+n'), (',', ','), ('até', '>S+adv'), ('porque', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('o', '>N+art'), ('terreno', 'H+n'), (',', ','), ('a', '>N+art'), ('capacidade', 'H+n'), ('de', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), ('está', 'P+v-fin'), ('cada_vez_mais', '>A+adv'), ('enfraquecida', 'H+v-pcp'), ('.', '.')]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "#Observando por sentenças\n",
    "tagged_sents = floresta.tagged_sents()\n",
    "\n",
    "for sent in tagged_sents[:10]:\n",
    "    print(sent)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando sentenças simplificadas\n",
    "simplified_sents = [[(n, simple_tag(t)) for n, t in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(simplified_sents)\n",
    "\n",
    "num_sents = len(simplified_sents)\n",
    "len_train = int(num_sents*0.8)\n",
    "\n",
    "train = simplified_sents[:len_train]\n",
    "test = simplified_sents[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Treinando POS taggers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia default tagger 0.18786394306312967\n",
      "Acuracia unigram tagger 0.8811119327109714\n",
      "Acuracia bigram tagger 0.8945836029207875\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import accuracy\n",
    "\n",
    "tagger0 = nltk.DefaultTagger('n')\n",
    "\n",
    "print('Acuracia default tagger', tagger0.evaluate(test))\n",
    "\n",
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "\n",
    "print('Acuracia unigram tagger', tagger1.evaluate(test))\n",
    "\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "\n",
    "print('Acuracia bigram tagger', tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testando</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eu', 'pron-pers'), ('sou', 'v-fin'), ('felipe', 'n'), ('navarro', 'n')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger2.tag(tokens('eu sou felipe navarro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chunkers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Contrução de uma base de dados para um Chunker em portugues</h3>\n",
    "\n",
    "<a href=\"https://www.linguateca.pt/primeiroHAREM/harem_coleccaodourada_en.html\">Fonte inicial</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc>\n",
      "<docid>harem-871-07800</docid>\n",
      "<genero>web</genero>\n",
      "<origem>pt</origem>\n",
      "<texto>\n",
      "<organizacao tipo=\"instituicao\" morf=\"f,s\">abraco</organizacao> pagina principal\n",
      "<organizacao tipo=\"instituicao\" morf=\"f,s\">associacao de apoio a pessoas com vih/sida</organizacao>\n",
      "a <organizacao tipo=\"instituicao\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "raw_xml = open('datasets/ColeccaoDouradaHAREM.txt', 'r', encoding='latin-1').read()\n",
    "raw_xml = raw_xml.replace('|', ' ')\n",
    "raw_xml = normalize_text(raw_xml)\n",
    "print(raw_xml[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num textos 129\n",
      "....\n",
      "Num textos com pessoas 78\n"
     ]
    }
   ],
   "source": [
    "soup = Soup(raw_xml)\n",
    "all_textos = soup.findAll('texto')\n",
    "num_textos = len(all_textos)\n",
    "print('Num textos', num_textos)\n",
    "print('....')\n",
    "texts_with_people = []\n",
    "\n",
    "for ctext in all_textos:\n",
    "    people = ctext.findAll('pessoa')\n",
    "    if len(people) > 1:\n",
    "        texts_with_people.append([ctext.text, [p.text for p in people]])\n",
    "\n",
    "print('Num textos com pessoas', len(texts_with_people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nfernando ferreira\\n[click for a page in english]\\ncmaf- universidade de lisboa gabinete a2-31 avenida professor gama pinto, 2 telefone do gabinete: 217904893 p-1649-003 lisboa extensao interna: 4293 portugal email: ferferr@cii.fc.ul.pt departamento de matematica faculdade de ciencias universidade de lisboa cmaf\\napresentacao\\nbem vindos aminha pagina pessoal. sou professor associado do departamento de matematica da universidade de lisboa e membro do centro de matematica e aplicacoes fundamentais - cmaf. clique aqui para obter o meu cv.\\ninteresses academicos\\nlogica matematica, em especial teorias fracas da aritmetica e da analise. filosofia  e fundamentos de matematica . tenho um interesse amador (no sentido latino da palavra) por alguns problemas da filosofia antiga , particularmente no problema da falsidade em parmenides e platao. tambem escrevi alguns ensaios expositorios sobre temas da logica: clique aqui para os ver.\\nensino\\nno presente semestre dou aulas teorico-praticas de algebra 2, cadeira do segundo ano das licenciaturas em matematica. o professor jose perdigao dias da silva eo regente da cadeira.\\nno semestre passado fui responsavel pelas cadeiras de topologia e introducao aanalise funcional, do terceiro ano das licenciaturas em matematica, e de teoria da demonstracao, do \\nmestrado em matematica.\\nno ano passado ensinei a cadeira de logica matematica aos finalistas de matematica e licenciaturas relacionadas. clique aqui para\\nver a pagina desta cadeira. tambem dei a cadeira logica de primeira-ordem ao primeiro ano das licenciaturas em informatica e engenharia da linguagem e do conhecimento. a pagina web desta cadeira ainda se encontra disponivel on-line em html://www.alf1.cii.fc.ul.pt/~ferferr/lpo.html .\\ntambem colaboro no mestrado em filosofia da linguagem e da consciencia da faculdade de letras.\\neventos\\nde 25 a 28 de junho decorrera em lisboa, no cmaf, a school on real algebraic and analytic geometry and o-minimal structures .\\nas quintas-feiras decorre o seminario de logica matematica (slm), organizado por mim e pelo professor narciso garcia do instituto superior tecnico. se quiser ter noticias regulares sobre o slm, por favor contacte-me.\\nvaria\\nsou co-editor da disputatio , uma revista de filosofia analitica.\\n',\n",
       " ['fernando ferreira',\n",
       "  'professor associado',\n",
       "  'professor jose perdigao dias da silva',\n",
       "  'professor narciso garcia']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_people[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fernando', 'B-nome'],\n",
       " ['ferreira', 'I-nome'],\n",
       " ['', 'O'],\n",
       " ['click', 'O'],\n",
       " ['for', 'O'],\n",
       " ['a', 'O'],\n",
       " ['page', 'O'],\n",
       " ['in', 'O'],\n",
       " ['english', 'O'],\n",
       " ['', 'O'],\n",
       " ['cmaf-', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['gabinete', 'O'],\n",
       " ['a2-31', 'O'],\n",
       " ['avenida', 'O'],\n",
       " ['professor', 'O'],\n",
       " ['gama', 'O'],\n",
       " ['pinto', 'O'],\n",
       " ['', 'O'],\n",
       " ['2', 'O'],\n",
       " ['telefone', 'O'],\n",
       " ['do', 'O'],\n",
       " ['gabinete', 'O'],\n",
       " ['', 'O'],\n",
       " ['217904893', 'O'],\n",
       " ['p-1649-003', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['extensao', 'O'],\n",
       " ['interna', 'O'],\n",
       " ['', 'O'],\n",
       " ['4293', 'O'],\n",
       " ['portugal', 'O'],\n",
       " ['email', 'O'],\n",
       " ['', 'O'],\n",
       " ['ferferr', 'O'],\n",
       " ['', 'O'],\n",
       " ['cii.fc.ul.pt', 'O'],\n",
       " ['departamento', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['faculdade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['ciencias', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['apresentacao', 'O'],\n",
       " ['bem', 'O'],\n",
       " ['vindos', 'O'],\n",
       " ['aminha', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['pessoal', 'O'],\n",
       " ['.', 'O'],\n",
       " ['sou', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['associado', 'I-nome'],\n",
       " ['do', 'O'],\n",
       " ['departamento', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['da', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['e', 'O'],\n",
       " ['membro', 'O'],\n",
       " ['do', 'O'],\n",
       " ['centro', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['aplicacoes', 'O'],\n",
       " ['fundamentais', 'O'],\n",
       " ['-', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['.', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['obter', 'O'],\n",
       " ['o', 'O'],\n",
       " ['meu', 'O'],\n",
       " ['cv', 'O'],\n",
       " ['.', 'O'],\n",
       " ['interesses', 'O'],\n",
       " ['academicos', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['em', 'O'],\n",
       " ['especial', 'O'],\n",
       " ['teorias', 'O'],\n",
       " ['fracas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['aritmetica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['da', 'O'],\n",
       " ['analise', 'O'],\n",
       " ['.', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['e', 'O'],\n",
       " ['fundamentos', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tenho', 'O'],\n",
       " ['um', 'O'],\n",
       " ['interesse', 'O'],\n",
       " ['amador', 'O'],\n",
       " ['', 'O'],\n",
       " ['no', 'O'],\n",
       " ['sentido', 'O'],\n",
       " ['latino', 'O'],\n",
       " ['da', 'O'],\n",
       " ['palavra', 'O'],\n",
       " ['', 'O'],\n",
       " ['por', 'O'],\n",
       " ['alguns', 'O'],\n",
       " ['problemas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['antiga', 'O'],\n",
       " ['', 'O'],\n",
       " ['particularmente', 'O'],\n",
       " ['no', 'O'],\n",
       " ['problema', 'O'],\n",
       " ['da', 'O'],\n",
       " ['falsidade', 'O'],\n",
       " ['em', 'O'],\n",
       " ['parmenides', 'O'],\n",
       " ['e', 'O'],\n",
       " ['platao', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['escrevi', 'O'],\n",
       " ['alguns', 'O'],\n",
       " ['ensaios', 'O'],\n",
       " ['expositorios', 'O'],\n",
       " ['sobre', 'O'],\n",
       " ['temas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['os', 'O'],\n",
       " ['ver', 'O'],\n",
       " ['.', 'O'],\n",
       " ['ensino', 'O'],\n",
       " ['no', 'O'],\n",
       " ['presente', 'O'],\n",
       " ['semestre', 'O'],\n",
       " ['dou', 'O'],\n",
       " ['aulas', 'O'],\n",
       " ['teorico-praticas', 'O'],\n",
       " ['de', 'O'],\n",
       " ['algebra', 'O'],\n",
       " ['2', 'O'],\n",
       " ['', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['do', 'O'],\n",
       " ['segundo', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['o', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['jose', 'I-nome'],\n",
       " ['perdigao', 'I-nome'],\n",
       " ['dias', 'I-nome'],\n",
       " ['da', 'I-nome'],\n",
       " ['silva', 'I-nome'],\n",
       " ['eo', 'O'],\n",
       " ['regente', 'O'],\n",
       " ['da', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['.', 'O'],\n",
       " ['no', 'O'],\n",
       " ['semestre', 'O'],\n",
       " ['passado', 'O'],\n",
       " ['fui', 'O'],\n",
       " ['responsavel', 'O'],\n",
       " ['pelas', 'O'],\n",
       " ['cadeiras', 'O'],\n",
       " ['de', 'O'],\n",
       " ['topologia', 'O'],\n",
       " ['e', 'O'],\n",
       " ['introducao', 'O'],\n",
       " ['aanalise', 'O'],\n",
       " ['funcional', 'O'],\n",
       " ['', 'O'],\n",
       " ['do', 'O'],\n",
       " ['terceiro', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['e', 'O'],\n",
       " ['de', 'O'],\n",
       " ['teoria', 'O'],\n",
       " ['da', 'O'],\n",
       " ['demonstracao', 'O'],\n",
       " ['', 'O'],\n",
       " ['do', 'O'],\n",
       " ['mestrado', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['no', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['passado', 'O'],\n",
       " ['ensinei', 'O'],\n",
       " ['a', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['de', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['aos', 'O'],\n",
       " ['finalistas', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['relacionadas', 'O'],\n",
       " ['.', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['ver', 'O'],\n",
       " ['a', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['desta', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['dei', 'O'],\n",
       " ['a', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['de', 'O'],\n",
       " ['primeira-ordem', 'O'],\n",
       " ['ao', 'O'],\n",
       " ['primeiro', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['informatica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['engenharia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['linguagem', 'O'],\n",
       " ['e', 'O'],\n",
       " ['do', 'O'],\n",
       " ['conhecimento', 'O'],\n",
       " ['.', 'O'],\n",
       " ['a', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['web', 'O'],\n",
       " ['desta', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['ainda', 'O'],\n",
       " ['se', 'O'],\n",
       " ['encontra', 'O'],\n",
       " ['disponivel', 'O'],\n",
       " ['on-line', 'O'],\n",
       " ['em', 'O'],\n",
       " ['html', 'O'],\n",
       " ['', 'O'],\n",
       " ['//www.alf1.cii.fc.ul.pt/~ferferr/lpo.html', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['colaboro', 'O'],\n",
       " ['no', 'O'],\n",
       " ['mestrado', 'O'],\n",
       " ['em', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['linguagem', 'O'],\n",
       " ['e', 'O'],\n",
       " ['da', 'O'],\n",
       " ['consciencia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['faculdade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['letras', 'O'],\n",
       " ['.', 'O'],\n",
       " ['eventos', 'O'],\n",
       " ['de', 'O'],\n",
       " ['25', 'O'],\n",
       " ['a', 'O'],\n",
       " ['28', 'O'],\n",
       " ['de', 'O'],\n",
       " ['junho', 'O'],\n",
       " ['decorrera', 'O'],\n",
       " ['em', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['', 'O'],\n",
       " ['no', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['', 'O'],\n",
       " ['a', 'O'],\n",
       " ['school', 'O'],\n",
       " ['on', 'O'],\n",
       " ['real', 'O'],\n",
       " ['algebraic', 'O'],\n",
       " ['and', 'O'],\n",
       " ['analytic', 'O'],\n",
       " ['geometry', 'O'],\n",
       " ['and', 'O'],\n",
       " ['o-minimal', 'O'],\n",
       " ['structures', 'O'],\n",
       " ['.', 'O'],\n",
       " ['as', 'O'],\n",
       " ['quintas-feiras', 'O'],\n",
       " ['decorre', 'O'],\n",
       " ['o', 'O'],\n",
       " ['seminario', 'O'],\n",
       " ['de', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['slm', 'O'],\n",
       " ['', 'O'],\n",
       " ['', 'O'],\n",
       " ['organizado', 'O'],\n",
       " ['por', 'O'],\n",
       " ['mim', 'O'],\n",
       " ['e', 'O'],\n",
       " ['pelo', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['narciso', 'I-nome'],\n",
       " ['garcia', 'I-nome'],\n",
       " ['do', 'O'],\n",
       " ['instituto', 'O'],\n",
       " ['superior', 'O'],\n",
       " ['tecnico', 'O'],\n",
       " ['.', 'O'],\n",
       " ['se', 'O'],\n",
       " ['quiser', 'O'],\n",
       " ['ter', 'O'],\n",
       " ['noticias', 'O'],\n",
       " ['regulares', 'O'],\n",
       " ['sobre', 'O'],\n",
       " ['o', 'O'],\n",
       " ['slm', 'O'],\n",
       " ['', 'O'],\n",
       " ['por', 'O'],\n",
       " ['favor', 'O'],\n",
       " ['contacte-me', 'O'],\n",
       " ['.', 'O'],\n",
       " ['varia', 'O'],\n",
       " ['sou', 'O'],\n",
       " ['co-editor', 'O'],\n",
       " ['da', 'O'],\n",
       " ['disputatio', 'O'],\n",
       " ['', 'O'],\n",
       " ['uma', 'O'],\n",
       " ['revista', 'O'],\n",
       " ['de', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['analitica', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def create_tagged_people_names(name, tag_name):\n",
    "    tokens = nltk.word_tokenize(name)\n",
    "    first_token = tokens[0] +'|B-' +tag_name\n",
    "    \n",
    "    middle_tokens = tokens[1:]\n",
    "    return ' '.join([first_token] + [t+'|I-'+tag_name for t in middle_tokens]) + ' '\n",
    "\n",
    "create_tagged_people_names('fernando ferreira', 'nome')\n",
    "\n",
    "def create_fully_tagged_text(full_text, names, tag_name):\n",
    "    tagged_names = [create_tagged_people_names(n, tag_name) for n in names]\n",
    "    modified_text = full_text\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        modified_text = modified_text.replace(name, tagged_names[i])\n",
    "        \n",
    "    tokens = nltk.word_tokenize(modified_text)\n",
    "    modified_tokens = []\n",
    "    for t in tokens:\n",
    "        if '|' in t:\n",
    "            modified_tokens.append(t)\n",
    "        else:\n",
    "            modified_tokens.append(t + '|O')\n",
    "    return ' '.join(modified_tokens)\n",
    "\n",
    "def create_separated_tag_text(tagged_text):\n",
    "    tokens = nltk.word_tokenize(tagged_text)\n",
    "    return [x.split('|') for x in tokens if len(x.split('|')) > 1]\n",
    "\n",
    "def process_entry(entry, tag_name):\n",
    "    text = entry[0]\n",
    "    names = entry[1]\n",
    "    ftt = create_fully_tagged_text(text, names, tag_name)\n",
    "    return create_separated_tag_text(ftt)\n",
    "\n",
    "process_entry(texts_with_people[0], 'nome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_entries = [process_entry(t, 'nome') for t in texts_with_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for_words(sentence, index):\n",
    "    current_word = sentence[index]\n",
    "    prefix3 = current_word[:3]\n",
    "    sufix3 = current_word[-3:]\n",
    "    if index == 0:\n",
    "        prev_word = ''\n",
    "    else:\n",
    "        prev_word = sentence[index - 1]\n",
    "    feats = {'word': current_word,\n",
    "            'prefix3': prefix3,\n",
    "            'sufix3': sufix3,\n",
    "            'prev_word': prev_word}\n",
    "    return feats\n",
    "\n",
    "def processed_entry_to_feats_and_targets(processed_entry):\n",
    "    sentence = [w[0] for w in processed_entry]\n",
    "    feats_and_targets = [[create_features_for_words(sentence, i), w[1]] for i, w in enumerate(processed_entry)]\n",
    "    return feats_and_targets\n",
    "\n",
    "new_processed = []\n",
    "for pe in processed_entries:\n",
    "    new_processed += processed_entry_to_feats_and_targets(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feats = [np[0] for np in new_processed]\n",
    "Targets = [np[1] for np in new_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-nome', 'I-nome', 'O'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dataset 74211\n"
     ]
    }
   ],
   "source": [
    "Feats = [np[0] for np in new_processed]\n",
    "Targets = [np[1] for np in new_processed]\n",
    "\n",
    "print('Len dataset', len(new_processed))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "F_train, F_test, T_train, T_test = train_test_split(Feats, Targets, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Targets,\n",
    "                                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_target(target):\n",
    "    if target == 'O':\n",
    "        return 0\n",
    "    elif target == 'B-nome':\n",
    "        return 1\n",
    "    elif target == 'I-nome':\n",
    "        return 2\n",
    "    \n",
    "def convert_target_index(target_index):\n",
    "    if target_index == 0:\n",
    "        return 'O'\n",
    "    elif target_index == 1:\n",
    "        return 'B-nome'\n",
    "    elif target_index == 2:\n",
    "        return 'I-nome'\n",
    "\n",
    "T_train = [convert_target(t) for t in T_train]\n",
    "T_test = [convert_target(t) for t in T_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform(F_train)\n",
    "X_test = vectorizer.transform(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, T_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusao [[20708   222    97]\n",
      " [  156   785    16]\n",
      " [  125    19   136]]\n",
      "Precisao [0.98661203 0.76510721 0.54618474]\n",
      "Recall [0.98482903 0.82027168 0.48571429]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(T_test, prediction)\n",
    "print('Matriz de confusao', cm)\n",
    "\n",
    "precision = precision_score(T_test, prediction, average=None)\n",
    "recall = recall_score(T_test, prediction, average=None)\n",
    "\n",
    "print('Precisao', precision)\n",
    "print('Recall', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garantindo o bom comportamento do modelo em termos de notacao IOB\n",
    "\n",
    "def create_features_for_words_sl(sentence, index, previous_classification):\n",
    "    current_word = sentence[index]\n",
    "    prefix3 = current_word[:3]\n",
    "    sufix3 = current_word[-3:]\n",
    "    if index == 0:\n",
    "        prev_word = ''\n",
    "    else:\n",
    "        prev_word = sentence[index - 1]\n",
    "    feats = {'word': current_word,\n",
    "            'prefix3': prefix3,\n",
    "            'sufix3': sufix3,\n",
    "            'prev_word': prev_word,\n",
    "            'previous_classification': previous_classification}\n",
    "    return feats\n",
    "\n",
    "def processed_entry_to_feats_and_targets_sl(processed_entry):\n",
    "    sentence = [w[0] for w in processed_entry]\n",
    "    tags = [w[1] for w in processed_entry]\n",
    "    \n",
    "    previous_tag = 'O'\n",
    "    feats_and_targets = []\n",
    "    for i, w in enumerate(processed_entry):\n",
    "        feats_and_targets.append(create_features_for_words_sl(sentence, i, previous_tag))\n",
    "        previous_tag = tags[i]\n",
    "    return feats_and_targets\n",
    "\n",
    "new_processed_sl = []\n",
    "for pe in processed_entries:\n",
    "    new_processed_sl += processed_entry_to_feats_and_targets_sl(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusao [[20677   278    72]\n",
      " [  130   810    17]\n",
      " [  131    14   135]]\n",
      "Precisao [0.98753463 0.73502722 0.60267857]\n",
      "Recall [0.98335473 0.84639498 0.48214286]\n"
     ]
    }
   ],
   "source": [
    "Feats_sl = [np[0] for np in new_processed]\n",
    "Targets_sl = [np[1] for np in new_processed]\n",
    "\n",
    "F_train_sl, F_test_sl, T_train_sl, T_test_sl = train_test_split(Feats_sl, Targets_sl, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Targets_sl,\n",
    "                                                                    shuffle=True)\n",
    "\n",
    "\n",
    "T_train_sl = [convert_target(t) for t in T_train_sl]\n",
    "T_test_sl = [convert_target(t) for t in T_test_sl]\n",
    "\n",
    "vectorizer_sl = DictVectorizer()\n",
    "X_train_sl = vectorizer_sl.fit_transform(F_train_sl)\n",
    "X_test_sl = vectorizer_sl.transform(F_test_sl)\n",
    "\n",
    "model_sl = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model_sl.fit(X_train_sl, T_train_sl)\n",
    "\n",
    "prediction_sl = model_sl.predict(X_test_sl)\n",
    "\n",
    "cm_sl = confusion_matrix(T_test_sl, prediction_sl)\n",
    "print('Matriz de confusao', cm_sl)\n",
    "\n",
    "precision_sl = precision_score(T_test_sl, prediction_sl, average=None)\n",
    "recall_sl = recall_score(T_test_sl, prediction_sl, average=None)\n",
    "\n",
    "print('Precisao', precision_sl)\n",
    "print('Recall', recall_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction_sl(text, classifier, vectorizer):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    last_prediction = 'O'\n",
    "    predictions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        feats = create_features_for_words_sl(tokens, i, last_prediction)\n",
    "        vect_feats = vectorizer.transform([feats])\n",
    "        prediction = convert_target_index(classifier.predict(vect_feats)[0])\n",
    "        last_prediction = prediction\n",
    "        predictions.append((token, prediction))\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('o', 'O'), ('meu', 'O'), ('nome', 'O'), ('eh', 'O'), ('fernando', 'O'), ('carlos', 'B-nome'), ('da', 'I-nome'), ('silva', 'I-nome'), (',', 'O'), ('entendeu', 'O'), ('?', 'O')]\n"
     ]
    }
   ],
   "source": [
    "prediction = make_prediction_sl('o meu nome eh fernando carlos da silva, entendeu?', model_sl, vectorizer_sl)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
