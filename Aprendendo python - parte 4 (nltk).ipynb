{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Nltk</h1>\n",
    "\n",
    "O Nltk é uma biblioteca para processamento de texto natural em python. Com ela é possível realizar classificação de textos, treinar chunkers, entre outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, uma função bastante útil para o processamento de textos em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def normalize_text(text, to_lower=True):\n",
    "    text = normalize('NFKD', text).encode('ASCII', 'ignore')\n",
    "\n",
    "    final_text = text\n",
    "\n",
    "    if to_lower:\n",
    "        final_text = text.lower()\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ola, tudo bem? como voce vai?'\n",
      "b'Ola, tudo bem? como voce vai?'\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text('OlÁ, tudo bem? como você vai?'))\n",
    "print(normalize_text('Olá, tudo bem? como você vai?', to_lower=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao realizar a normalização de textos, considere excluir casos como o verbo de ligação \"é\" e o símbolo de parágrafo &para;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baixando corpora e pacotes úteis</h2\n",
    "<br><br>\n",
    "Vamos agora baixar exemplos de textos em português para trabalhar e alguns pacotes que serão úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Baixar: book\n",
    "#Baixar: all-corpora\n",
    "#Baixar: floresta\n",
    "#Baixar: machado\n",
    "#Baixar: punkt\n",
    "#Baixar: stopwords\n",
    "\n",
    "#Se houverem problemas com a interface. Abrir o terminal do Jupyter e inserir os comandos:\n",
    "#python -m nltk downloader book\n",
    "#python -m nltk downloader all-corpora\n",
    "#python -m nltk.downloader floresta\n",
    "#python -m nltk.downloader machado\n",
    "#python -m nltk.downloader punkt\n",
    "#python -m nltk.downloader stopwords\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Carregando textos e conhecendo funções úteis do Nltk</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segmentação de texto</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenização</h3>\n",
    "<br>\n",
    "Tokenização é o processo de dividir uma frase em palavras particulares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meu', 'nome', 'é', 'felipe', 'navarro']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('meu nome é felipe navarro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Segmentação em sentenças (sent tokenization)</h3>\n",
    "<br>\n",
    "É o processo de quebrar um texto grande em frases individuais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meu nome é João da Silva e gosto de passear no feriado.',\n",
       " 'Recentemente comprei um carro, que quebrou ontem']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "\n",
    "texto = 'Meu nome é João da Silva e gosto de passear no feriado. Recentemente comprei um carro, que quebrou ontem'\n",
    "\n",
    "sent_tokenizer.tokenize(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para simplificar vamos criar funções\n",
    "\n",
    "def tokens(my_str):\n",
    "    return nltk.word_tokenize(my_str)\n",
    "\n",
    "def sent_seg(my_str):\n",
    "    return sent_tokenizer.tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classificação sintática</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualizando corpus taggeado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', 'é', 'um', 'ex-libris', 'de', 'a']\n",
      "----------\n",
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj'), ('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import floresta\n",
    "#Visualizando as palavras\n",
    "print(floresta.words()[:10])\n",
    "\n",
    "print('----------')\n",
    "#Com tags\n",
    "print(floresta.tagged_words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'art'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_tag(tag):\n",
    "    try:\n",
    "        return tag.split('+')[1]\n",
    "    except:\n",
    "        return tag\n",
    "\n",
    "simple_tag('>N+art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj')]\n",
      "--------\n",
      "[('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), ('algarvia', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('uma', 'H+num'), ('de', 'H+prp'), ('as', '>N+art'), ('mais', '>A+adv'), ('antigas', 'H+adj'), ('discotecas', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Algarve', 'H+prop'), (',', ','), ('situada', 'P+v-pcp'), ('em', 'H+prp'), ('Albufeira', 'P<+prop'), (',', ','), ('que', 'SUBJ+pron-indp'), ('continua', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('manter', 'MV+v-inf'), ('os', '>N+art'), ('traços', 'H+n'), ('decorativos', 'N<+adj'), ('e', 'CO+conj-c'), ('as', '>N+art'), ('clientelas', 'H+n'), ('de', 'H+prp'), ('sempre', 'P<+adv'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('um_pouco', 'ADVL+adv'), ('a', '>N+art'), ('versão', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('espécie', 'H+n'), ('de', 'H+prp'), ('«', '«'), ('outro', '>N+pron-det'), ('lado', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), (',', ','), ('a', 'H+prp'), ('meio', '>N+adj'), ('caminho', 'H+n'), ('entre', 'H+prp'), ('os', '>N+art'), ('devaneios', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('fauna', 'H+n'), ('periférica', 'N<+adj'), (',', ','), ('seja', 'P+v-fin'), ('de', 'H+prp'), ('Lisboa', 'CJT+prop'), (',', ','), ('Londres', 'CJT+prop'), (',', ','), ('Dublin', 'CJT+prop'), ('ou', 'CO+conj-c'), ('Faro', 'CJT+n'), ('e', 'CO+conj-c'), ('Portimão', 'CJT+prop'), (',', ','), ('e', 'CO+conj-c'), ('a', '>N+art'), ('postura', 'H+n'), ('circunspecta', 'N<+adj'), ('de', 'H+prp'), ('os', '>N+art'), ('fiéis', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('casa', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('de', 'H+prp'), ('ela', 'P<+pron-pers'), ('esperam', 'P+v-fin'), ('a', '>N+art'), ('música', 'H+n'), ('«', '«'), ('geracionista', 'N<+n'), ('de', 'H+prp'), ('os', '>A+art'), ('60', 'H+num'), ('ou', 'CO+conj-c'), ('de', 'H+prp'), ('os', '>A+art'), ('70', 'H+num'), ('.', '.')]\n",
      "--------\n",
      "[('Não', 'ADVL+adv'), ('deixa', 'AUX+v-fin'), ('de', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('tempos', 'H+n'), ('que', 'SUBJ+pron-indp'), ('correm', 'P+v-fin'), (',', ','), ('um', '>N+art'), ('certo', '>N+pron-det'), ('«', '«'), ('very_typical', 'H+n'), ('algarvio', 'N<+adj'), (',', ','), ('cabeça', 'H+n'), ('de', 'H+prp'), ('cartaz', 'P<+n'), ('para', 'H+prp'), ('os', 'H+pron-det'), ('que', 'SUBJ+pron-indp'), ('querem', 'P+v-fin'), ('fugir', 'P+v-inf'), ('a', 'H+prp'), ('algumas', '>N+pron-det'), ('movimentações', 'H+n'), ('nocturnas', 'N<+adj'), ('já', '>A+adv'), ('a', 'H+prp'), ('caminho', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('ritualização', 'H+n'), ('de', 'H+prp'), ('massas', 'P<+n'), (',', ','), ('de', 'H+prp'), ('o', '>N+art'), ('género', 'H+n'), ('«', '«'), ('vamos', 'P+v-fin'), ('todos', 'SUBJ+pron-det'), ('a', 'H+prp'), ('o', '>N+art'), ('Calypso', 'H+prop'), ('e', 'CO+conj-c'), ('encontramos-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('em', 'H+prp'), ('a', '>N+art'), ('Locomia', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('assim', 'ADVL+adv'), (',', ','), ('a', 'H+prp'), ('os', '>N+art'), ('2,5', '>N+num'), ('milhões', 'H+n'), ('que', 'ACC+pron-indp'), ('o', '>N+art'), ('Ministério_do_Planeamento_e_Administração_do_Território', 'H+prop'), ('já', 'ADVL+adv'), ('gasta', 'P+v-fin'), ('em', 'H+prp'), ('o', '>N+art'), ('pagamento', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('pessoal', 'H+n'), ('afecto', 'P+v-pcp'), ('a', 'H+prp'), ('estes', '>N+pron-det'), ('organismos', 'H+n'), (',', ','), ('vêm', 'P+v-fin'), ('juntar-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('os', '>N+art'), ('montantes', 'H+n'), ('de', 'H+prp'), ('as', '>N+art'), ('obras', 'H+n'), ('propriamente', '>A+adv'), ('ditas', 'H+adj'), (',', ','), ('que', 'ACC+pron-indp'), ('os', '>N+art'), ('municípios', 'H+n'), (',', ','), ('já', '>A+adv'), ('com', 'H+prp'), ('projectos', 'H+n'), ('em', 'H+prp'), ('a', '>N+art'), ('mão', 'H+n'), (',', ','), ('vêm', 'AUX+v-fin'), ('reivindicar', 'MV+v-inf'), ('junto', 'H+adv'), ('de', 'H+prp'), ('o', '>N+art'), ('Executivo', 'H+n'), (',', ','), ('como', 'H+prp'), ('salienta', 'P+v-fin'), ('aquele', '>N+pron-det'), ('membro', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Governo', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('o', '>N+art'), ('dinheiro', 'H+n'), ('«', '«'), ('não', 'ADVL+adv'), ('falta', 'P+v-fin'), ('só', '>A+adv'), ('a', 'H+prp'), ('as', '>N+art'), ('câmaras', 'H+n'), (',', ','), ('lembra', 'P+v-fin'), ('o', '>N+art'), ('secretário_de_Estado', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('considera', 'P+v-fin'), ('que', 'SUB+conj-s'), ('a', '>N+art'), ('solução', 'H+n'), ('para', 'H+prp'), ('as', '>N+art'), ('autarquias', 'H+n'), ('é', 'P+v-fin'), ('«', '«'), ('especializarem-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('em', 'H+prp'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('Mas', 'CO+conj-c'), ('como', 'ADVL+adv'), (',', ','), ('se', 'SUB+conj-s'), ('muitas', 'SUBJ+pron-det'), ('não', 'ADVL+adv'), ('dispõem', 'P+v-fin'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('seus', '>N+pron-det'), ('quadros', 'H+n'), (',', ','), ('de', 'H+prp'), ('os', '>N+art'), ('técnicos', 'H+n'), ('necessários', 'N<+adj'), ('?', '?')]\n",
      "--------\n",
      "[('«', '«'), ('Encomendem-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('a', 'H+prp'), ('projectistas', 'H+n'), ('de_fora', 'N<+pp'), ('porque', 'SUB+conj-s'), (',', ','), ('se', 'SUB+conj-s'), ('as', '>N+art'), ('obras', 'H+n'), ('vierem', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('ser', 'AUX+v-inf'), ('financiadas', 'MV+v-pcp'), (',', ','), ('eles', 'SUBJ+pron-pers'), ('até', 'ADVL+adv'), ('saem', 'P+v-fin'), ('de_graça', 'ADVS+pp'), (',', ','), ('já_que', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('esse', '>N+pron-det'), ('caso', 'H+n'), (',', ','), ('«', '«'), ('os', '>N+art'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('pagam', 'P+v-fin'), ('os', '>N+art'), ('projectos', 'H+n'), (',', ','), ('o', '>N+art'), ('mesmo', 'H+pron-det'), ('não', 'ADVL+adv'), ('acontecendo', 'P+v-ger'), ('quando', 'ADVL+adv'), ('eles', 'SUBJ+pron-pers'), ('são', 'AUX+v-fin'), ('feitos', 'MV+v-pcp'), ('por', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), (',', ','), ('dado', 'P+v-pcp'), ('serem', 'P+v-inf'), ('organismos', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Estado', 'H+n'), ('.', '.')]\n",
      "--------\n",
      "[('Essa', 'SUBJ+pron-det'), ('poderá', 'AUX+v-fin'), ('vir', 'AUX+v-inf'), ('a', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), ('uma', '>N+art'), ('hipótese', 'H+n'), (',', ','), ('até', '>S+adv'), ('porque', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('o', '>N+art'), ('terreno', 'H+n'), (',', ','), ('a', '>N+art'), ('capacidade', 'H+n'), ('de', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), ('está', 'P+v-fin'), ('cada_vez_mais', '>A+adv'), ('enfraquecida', 'H+v-pcp'), ('.', '.')]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "#Observando por sentenças\n",
    "tagged_sents = floresta.tagged_sents()\n",
    "\n",
    "for sent in tagged_sents[:10]:\n",
    "    print(sent)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando sentenças simplificadas\n",
    "simplified_sents = [[(n, simple_tag(t)) for n, t in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(simplified_sents)\n",
    "\n",
    "num_sents = len(simplified_sents)\n",
    "len_train = int(num_sents*0.8)\n",
    "\n",
    "train = simplified_sents[:len_train]\n",
    "test = simplified_sents[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Treinando POS taggers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia default tagger 0.18786394306312967\n",
      "Acuracia unigram tagger 0.8811119327109714\n",
      "Acuracia bigram tagger 0.8945836029207875\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import accuracy\n",
    "\n",
    "tagger0 = nltk.DefaultTagger('n')\n",
    "\n",
    "print('Acuracia default tagger', tagger0.evaluate(test))\n",
    "\n",
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "\n",
    "print('Acuracia unigram tagger', tagger1.evaluate(test))\n",
    "\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "\n",
    "print('Acuracia bigram tagger', tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testando</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eu', 'pron-pers'), ('sou', 'v-fin'), ('felipe', 'n'), ('navarro', 'n')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger2.tag(tokens('eu sou felipe navarro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chunkers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualizando corpus de Name entity recognition</h3>\n",
    "\n",
    "<a href=\"https://streamhacker.com/2008/12/29/how-to-train-a-nltk-chunker/\">Veja aqui</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
