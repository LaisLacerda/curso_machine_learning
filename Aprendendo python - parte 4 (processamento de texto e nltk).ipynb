{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exemplo de classificação de texto com Sklearn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw_file = open('datasets/reviews.json', 'r', encoding='utf-8').read()\n",
    "as_json = json.loads(raw_file)\n",
    "num_texts = len(as_json['paper'])\n",
    "\n",
    "print(num_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'preliminary_decision': 'accept',\n",
       " 'review': [{'confidence': '4',\n",
       "   'evaluation': '1',\n",
       "   'id': 1,\n",
       "   'lan': 'es',\n",
       "   'orientation': '0',\n",
       "   'remarks': '',\n",
       "   'text': '- El artículo aborda un problema contingente y muy relevante, e incluye tanto un diagnóstico nacional de uso de buenas prácticas como una solución (buenas prácticas concretas). - El lenguaje es adecuado.  - El artículo se siente como la concatenación de tres artículos diferentes: (1) resultados de una encuesta, (2) buenas prácticas de seguridad, (3) incorporación de buenas prácticas. - El orden de las secciones sería mejor si refleja este orden (la versión revisada es #2, #1, #3). - El artículo no tiene validación de ningún tipo, ni siquiera por evaluación de expertos.',\n",
       "   'timespan': '2010-07-05'},\n",
       "  {'confidence': '4',\n",
       "   'evaluation': '1',\n",
       "   'id': 2,\n",
       "   'lan': 'es',\n",
       "   'orientation': '1',\n",
       "   'remarks': '',\n",
       "   'text': 'El artículo presenta recomendaciones prácticas para el desarrollo de software seguro. Se describen las mejores prácticas recomendadas para desarrollar software que sea proactivo ante los ataques, y se realiza un análisis de costos de estas prácticas en desarrollo de software. Todo basado en una revisión de prácticas propuestas en la bibliografía y su contraste con datos obtenidos de una encuesta en empresas. Finalmente se recomienda una guía.  Sería ideal aplicar la guía propuesta a empresas no involucradas en la encuesta que sirvió para originarla de modo de poder evaluar su efectividad en forma independiente.',\n",
       "   'timespan': '2010-07-05'},\n",
       "  {'confidence': '5',\n",
       "   'evaluation': '1',\n",
       "   'id': 3,\n",
       "   'lan': 'es',\n",
       "   'orientation': '1',\n",
       "   'remarks': '',\n",
       "   'text': '- El tema es muy interesante y puede ser de mucha ayuda una guía para incorporar prácticas de seguridad. - La presentación (descripción, etapa y uso) de las 9 prácticas para el desarrollo de software seguro.  - El “estado real del desarrollo de software en Chile” (como lo indica en su paper) no se puede lograr con solamente 22 encuestas de un total de 50. - Presenta nueve tablas que corresponden a las prácticas para el desarrollo de software seguro, pero la guía presenta 10 prácticas. ¿explica por qué? - Sugiero mejorar la guía, el mayor aporte está en la secuencia de incorporación que propone.  Además, no debería explicar la práctica en Observaciones ni diferenciarla con otras prácticas en esa columna, sino que debería dar sugerencias de cómo aplicarla. - En el texto indica “Más adelante, se presentan además tres prácticas extras…” ¿cuáles son o no leí correctamente? - De acuerdo a formato, poner como mínimo 5 palabras clave. - Sugiero mencionar las prácticas antes de mostrar cada tabla. - Algunas referencias están incompletas, por ejemplo, falta año en referencia 17, falta año y tipo de evento en referencia 11, falta editorial en referencia 19 (¿es un libro?) - Algunos títulos llevan una coma dentro de las comillas, ejemplo, referencia 1',\n",
       "   'timespan': '2010-07-05'}]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observando os dados\n",
    "\n",
    "entries = [j for j in as_json['paper']]\n",
    "entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET classifications ['accept', 'no decision', 'reject', 'probably reject']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = [' '.join([x['text'] for x in j['review']]) for j in entries]\n",
    "classifications = [j['preliminary_decision'] for j in entries]\n",
    "\n",
    "class_set = list(set(classifications))\n",
    "print('SET classifications', class_set)\n",
    "\n",
    "numeric_classifications = [class_set.index(c) for c in classifications]\n",
    "\n",
    "Y = np.array(numeric_classifications).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Criação de features numéricas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_1 = CountVectorizer()\n",
    "vectorizer_2 = TfidfVectorizer()\n",
    "\n",
    "X_1 = vectorizer_1.fit_transform(texts)\n",
    "X_2 = vectorizer_2.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6704 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 192 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_1[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Teste dos classificadores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fnba/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy GaussianNB + CountVectorizer 0.5384615384615384\n",
      "Accuracy GaussianNB + Tfidf 0.5192307692307693\n",
      "Accuracy LogisticRegression + CountVectorizer 0.7115384615384616\n",
      "Accuracy LogisticRegression + Tfidf 0.6730769230769231\n",
      "Accuracy MLPClassifier + CountVectorizer 0.6730769230769231\n",
      "Accuracy MLPClassifier + Tfidf 0.6730769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(X_1, Y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    stratify=Y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_2, Y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    stratify=Y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "model1_1 = GaussianNB()\n",
    "model1_2 = GaussianNB()\n",
    "model2_1 = LogisticRegression()\n",
    "model2_2 = LogisticRegression()\n",
    "model3_1 = MLPClassifier((10,), activation='logistic')\n",
    "model3_2 = MLPClassifier((10,), activation='logistic')\n",
    "\n",
    "model1_1.fit(X_train_1.todense(), Y_train_1)\n",
    "model1_2.fit(X_train_2.todense(), Y_train_2)\n",
    "model2_1.fit(X_train_1, Y_train_1)\n",
    "model2_2.fit(X_train_2, Y_train_2)\n",
    "model3_1.fit(X_train_1, Y_train_1)\n",
    "model3_2.fit(X_train_2, Y_train_2)\n",
    "\n",
    "accuracy1_1 = model1_1.score(X_test_1.todense(), Y_test_1)\n",
    "accuracy1_2 = model1_2.score(X_test_2.todense(), Y_test_2)\n",
    "accuracy2_1 = model2_1.score(X_test_1, Y_test_1)\n",
    "accuracy2_2 = model2_2.score(X_test_2, Y_test_2)\n",
    "accuracy3_1 = model3_1.score(X_test_1, Y_test_1)\n",
    "accuracy3_2 = model3_2.score(X_test_2, Y_test_2)\n",
    "\n",
    "print('Accuracy GaussianNB + CountVectorizer', accuracy1_1)\n",
    "print('Accuracy GaussianNB + Tfidf', accuracy1_2)\n",
    "print('Accuracy LogisticRegression + CountVectorizer', accuracy2_1)\n",
    "print('Accuracy LogisticRegression + Tfidf', accuracy2_2)\n",
    "print('Accuracy MLPClassifier + CountVectorizer', accuracy3_1)\n",
    "print('Accuracy MLPClassifier + Tfidf', accuracy3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atividade: Substitua a representacao bag of words pela proporcao de cada palavra e realize os testes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atividade: Crie um classificador para diferenciar pessoas fisicas ou juridicas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atividade: Encontrar as palavras com maior ganho de informação</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atividade: Tentar melhorar o desempenho do classificador</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Nltk</h1>\n",
    "\n",
    "<a href=\"https://www.nltk.org/book/\">Referência sobre Nltk</a>\n",
    "\n",
    "O Nltk é uma biblioteca para processamento de texto natural em python. Com ela é possível realizar classificação de textos, treinar chunkers, entre outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, uma função bastante útil para o processamento de textos em português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "def normalize_text(text, to_lower=True):\n",
    "    text = normalize('NFKD', text).encode('ASCII', 'ignore')\n",
    "\n",
    "    final_text = text\n",
    "\n",
    "    if to_lower:\n",
    "        final_text = text.lower()\n",
    "        \n",
    "    return final_text.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola, tudo bem? como voce vai?\n",
      "Ola, tudo bem? como voce vai?\n"
     ]
    }
   ],
   "source": [
    "print(normalize_text('OlÁ, tudo bem? como você vai?'))\n",
    "print(normalize_text('Olá, tudo bem? como você vai?', to_lower=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao realizar a normalização de textos, considere excluir casos como o verbo de ligação \"é\" e o símbolo de parágrafo &para;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Baixando corpora e pacotes úteis</h2\n",
    "<br><br>\n",
    "Vamos agora baixar exemplos de textos em português para trabalhar e alguns pacotes que serão úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "#Baixar: book\n",
    "#Baixar: all-corpora\n",
    "#Baixar: floresta\n",
    "#Baixar: machado\n",
    "#Baixar: punkt\n",
    "#Baixar: stopwords\n",
    "\n",
    "#Se houverem problemas com a interface. Abrir o terminal do Jupyter e inserir os comandos:\n",
    "#python -m nltk downloader book\n",
    "#python -m nltk downloader all-corpora\n",
    "#python -m nltk.downloader floresta\n",
    "#python -m nltk.downloader machado\n",
    "#python -m nltk.downloader punkt\n",
    "#python -m nltk.downloader stopwords\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Carregando texto e funções úteis do NLTK</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "f = open('datasets/machado.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Cartomante HAMLET observa a Horácio que há mais cousas no céu e na terra do que sonha a nossa filo'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: A Cartomante HAMLET observa a Horácio que há...>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(f)\n",
    "my_txt = nltk.Text(tokens)\n",
    "my_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fazendo pesquisas textuais</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      " observa a Horácio que há mais cousas no céu\n"
     ]
    }
   ],
   "source": [
    "my_txt.concordance('observa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      "te sem demora , — repetia ele com os olhos no papel . Imaginariamente , viu a p\n",
      "avras estavam decoradas , diante dos olhos , fixas , ou então , — o que era ain\n"
     ]
    }
   ],
   "source": [
    "my_txt.concordance('olhos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Obtendo a distribuição de palavras</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist(my_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd['olhos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({',': 177, '.': 83, 'que': 58, 'a': 55, 'e': 51, 'de': 42, 'o': 29, ';': 26, 'Camilo': 20, 'as': 16, ...})"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Encontrando um texto dentro de outro</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 16), (32, 37)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "txt1 = 'Este eh um texto onde a palavra texto acontece mais de uma vez'\n",
    "[(x.start(), x.end()) for x in re.finditer('texto', txt1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopwords em português</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'a', 'o', 'que', 'e', 'do', 'da', 'em', 'um', 'para']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Segmentação de texto</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenização</h3>\n",
    "<br>\n",
    "Tokenização é o processo de dividir uma frase em palavras particulares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['meu', 'nome', 'é', 'felipe', 'navarro']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize('meu nome é felipe navarro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Segmentação em sentenças (sent tokenization)</h3>\n",
    "<br>\n",
    "É o processo de quebrar um texto grande em frases individuais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Meu nome é João da Silva e gosto de passear no feriado.',\n",
       " 'Recentemente comprei um carro, que quebrou ontem']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenizer=nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "\n",
    "texto = 'Meu nome é João da Silva e gosto de passear no feriado. Recentemente comprei um carro, que quebrou ontem'\n",
    "\n",
    "sent_tokenizer.tokenize(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para simplificar vamos criar funções\n",
    "\n",
    "def tokens(my_str):\n",
    "    return nltk.word_tokenize(my_str)\n",
    "\n",
    "def sent_seg(my_str):\n",
    "    return sent_tokenizer.tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming em portugues</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'copi'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "stemmer.stem(\"copiar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Funções úteis em portugues com spacy</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rodar o comando pip install spacy\n",
    "#Rodar o comando python -m spacy download pt\n",
    "import spacy\n",
    "nlp = spacy.load('pt')\n",
    "my_doc = nlp('Encontrarei você no mesmo lugar onde ele estava. Tudo bem?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Separação em tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Encontrarei',\n",
       " 'você',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'lugar',\n",
       " 'onde',\n",
       " 'ele',\n",
       " 'estava.',\n",
       " 'Tudo',\n",
       " 'bem?']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doc.text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Encontrarei',\n",
       " 'você',\n",
       " 'no',\n",
       " 'mesmo',\n",
       " 'lugar',\n",
       " 'onde',\n",
       " 'ele',\n",
       " 'estava',\n",
       " '.',\n",
       " 'Tudo',\n",
       " 'bem',\n",
       " '?']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in my_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classificação sintática com spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Encontrarei', 'VERB'),\n",
       " ('você', 'PRON'),\n",
       " ('no', 'VERB'),\n",
       " ('mesmo', 'DET'),\n",
       " ('lugar', 'NOUN'),\n",
       " ('onde', 'ADV'),\n",
       " ('ele', 'PRON'),\n",
       " ('estava', 'VERB'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Tudo', 'PRON'),\n",
       " ('bem', 'ADV'),\n",
       " ('?', 'PUNCT')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token.orth_, token.pos_) for token in my_doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatizer em português</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encontrar', ',', 'encontrar', ',', 'encontrar']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_verb = nlp('encontrarei, encontraremos, encontro')\n",
    "[token.lemma_ for token in test_verb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reconhecimento de nome e entidade com o Spacy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Steve Jobs, Apple)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ner = nlp('Steve Jobs foi um dos criadores da Apple inc.')\n",
    "test_ner.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Steve Jobs, 'PER'), (Apple, 'ORG')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(entity, entity.label_) for entity in test_ner.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Classificação sintática</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualizando corpus taggeado</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', 'é', 'um', 'ex-libris', 'de', 'a']\n",
      "----------\n",
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj'), ('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import floresta\n",
    "#Visualizando as palavras\n",
    "print(floresta.words()[:10])\n",
    "\n",
    "print('----------')\n",
    "#Com tags\n",
    "print(floresta.tagged_words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'art'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_tag(tag):\n",
    "    try:\n",
    "        return tag.split('+')[1]\n",
    "    except:\n",
    "        return tag\n",
    "\n",
    "simple_tag('>N+art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Um', '>N+art'), ('revivalismo', 'H+n'), ('refrescante', 'N<+adj')]\n",
      "--------\n",
      "[('O', '>N+art'), ('7_e_Meio', 'H+prop'), ('é', 'P+v-fin'), ('um', '>N+art'), ('ex-libris', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), ('algarvia', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('uma', 'H+num'), ('de', 'H+prp'), ('as', '>N+art'), ('mais', '>A+adv'), ('antigas', 'H+adj'), ('discotecas', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Algarve', 'H+prop'), (',', ','), ('situada', 'P+v-pcp'), ('em', 'H+prp'), ('Albufeira', 'P<+prop'), (',', ','), ('que', 'SUBJ+pron-indp'), ('continua', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('manter', 'MV+v-inf'), ('os', '>N+art'), ('traços', 'H+n'), ('decorativos', 'N<+adj'), ('e', 'CO+conj-c'), ('as', '>N+art'), ('clientelas', 'H+n'), ('de', 'H+prp'), ('sempre', 'P<+adv'), ('.', '.')]\n",
      "--------\n",
      "[('É', 'P+v-fin'), ('um_pouco', 'ADVL+adv'), ('a', '>N+art'), ('versão', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('espécie', 'H+n'), ('de', 'H+prp'), ('«', '«'), ('outro', '>N+pron-det'), ('lado', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('noite', 'H+n'), (',', ','), ('a', 'H+prp'), ('meio', '>N+adj'), ('caminho', 'H+n'), ('entre', 'H+prp'), ('os', '>N+art'), ('devaneios', 'H+n'), ('de', 'H+prp'), ('uma', '>N+art'), ('fauna', 'H+n'), ('periférica', 'N<+adj'), (',', ','), ('seja', 'P+v-fin'), ('de', 'H+prp'), ('Lisboa', 'CJT+prop'), (',', ','), ('Londres', 'CJT+prop'), (',', ','), ('Dublin', 'CJT+prop'), ('ou', 'CO+conj-c'), ('Faro', 'CJT+n'), ('e', 'CO+conj-c'), ('Portimão', 'CJT+prop'), (',', ','), ('e', 'CO+conj-c'), ('a', '>N+art'), ('postura', 'H+n'), ('circunspecta', 'N<+adj'), ('de', 'H+prp'), ('os', '>N+art'), ('fiéis', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('casa', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('de', 'H+prp'), ('ela', 'P<+pron-pers'), ('esperam', 'P+v-fin'), ('a', '>N+art'), ('música', 'H+n'), ('«', '«'), ('geracionista', 'N<+n'), ('de', 'H+prp'), ('os', '>A+art'), ('60', 'H+num'), ('ou', 'CO+conj-c'), ('de', 'H+prp'), ('os', '>A+art'), ('70', 'H+num'), ('.', '.')]\n",
      "--------\n",
      "[('Não', 'ADVL+adv'), ('deixa', 'AUX+v-fin'), ('de', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('tempos', 'H+n'), ('que', 'SUBJ+pron-indp'), ('correm', 'P+v-fin'), (',', ','), ('um', '>N+art'), ('certo', '>N+pron-det'), ('«', '«'), ('very_typical', 'H+n'), ('algarvio', 'N<+adj'), (',', ','), ('cabeça', 'H+n'), ('de', 'H+prp'), ('cartaz', 'P<+n'), ('para', 'H+prp'), ('os', 'H+pron-det'), ('que', 'SUBJ+pron-indp'), ('querem', 'P+v-fin'), ('fugir', 'P+v-inf'), ('a', 'H+prp'), ('algumas', '>N+pron-det'), ('movimentações', 'H+n'), ('nocturnas', 'N<+adj'), ('já', '>A+adv'), ('a', 'H+prp'), ('caminho', 'H+n'), ('de', 'H+prp'), ('a', '>N+art'), ('ritualização', 'H+n'), ('de', 'H+prp'), ('massas', 'P<+n'), (',', ','), ('de', 'H+prp'), ('o', '>N+art'), ('género', 'H+n'), ('«', '«'), ('vamos', 'P+v-fin'), ('todos', 'SUBJ+pron-det'), ('a', 'H+prp'), ('o', '>N+art'), ('Calypso', 'H+prop'), ('e', 'CO+conj-c'), ('encontramos-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('em', 'H+prp'), ('a', '>N+art'), ('Locomia', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('assim', 'ADVL+adv'), (',', ','), ('a', 'H+prp'), ('os', '>N+art'), ('2,5', '>N+num'), ('milhões', 'H+n'), ('que', 'ACC+pron-indp'), ('o', '>N+art'), ('Ministério_do_Planeamento_e_Administração_do_Território', 'H+prop'), ('já', 'ADVL+adv'), ('gasta', 'P+v-fin'), ('em', 'H+prp'), ('o', '>N+art'), ('pagamento', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('pessoal', 'H+n'), ('afecto', 'P+v-pcp'), ('a', 'H+prp'), ('estes', '>N+pron-det'), ('organismos', 'H+n'), (',', ','), ('vêm', 'P+v-fin'), ('juntar-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('os', '>N+art'), ('montantes', 'H+n'), ('de', 'H+prp'), ('as', '>N+art'), ('obras', 'H+n'), ('propriamente', '>A+adv'), ('ditas', 'H+adj'), (',', ','), ('que', 'ACC+pron-indp'), ('os', '>N+art'), ('municípios', 'H+n'), (',', ','), ('já', '>A+adv'), ('com', 'H+prp'), ('projectos', 'H+n'), ('em', 'H+prp'), ('a', '>N+art'), ('mão', 'H+n'), (',', ','), ('vêm', 'AUX+v-fin'), ('reivindicar', 'MV+v-inf'), ('junto', 'H+adv'), ('de', 'H+prp'), ('o', '>N+art'), ('Executivo', 'H+n'), (',', ','), ('como', 'H+prp'), ('salienta', 'P+v-fin'), ('aquele', '>N+pron-det'), ('membro', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Governo', 'H+prop'), ('.', '.')]\n",
      "--------\n",
      "[('E', 'CO+conj-c'), ('o', '>N+art'), ('dinheiro', 'H+n'), ('«', '«'), ('não', 'ADVL+adv'), ('falta', 'P+v-fin'), ('só', '>A+adv'), ('a', 'H+prp'), ('as', '>N+art'), ('câmaras', 'H+n'), (',', ','), ('lembra', 'P+v-fin'), ('o', '>N+art'), ('secretário_de_Estado', 'H+n'), (',', ','), ('que', 'SUBJ+pron-indp'), ('considera', 'P+v-fin'), ('que', 'SUB+conj-s'), ('a', '>N+art'), ('solução', 'H+n'), ('para', 'H+prp'), ('as', '>N+art'), ('autarquias', 'H+n'), ('é', 'P+v-fin'), ('«', '«'), ('especializarem-', 'P+v-inf'), ('se', 'ACC+pron-pers'), ('em', 'H+prp'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('.', '.')]\n",
      "--------\n",
      "[('Mas', 'CO+conj-c'), ('como', 'ADVL+adv'), (',', ','), ('se', 'SUB+conj-s'), ('muitas', 'SUBJ+pron-det'), ('não', 'ADVL+adv'), ('dispõem', 'P+v-fin'), (',', ','), ('em', 'H+prp'), ('os', '>N+art'), ('seus', '>N+pron-det'), ('quadros', 'H+n'), (',', ','), ('de', 'H+prp'), ('os', '>N+art'), ('técnicos', 'H+n'), ('necessários', 'N<+adj'), ('?', '?')]\n",
      "--------\n",
      "[('«', '«'), ('Encomendem-', 'P+v-fin'), ('nos', 'ACC+pron-pers'), ('a', 'H+prp'), ('projectistas', 'H+n'), ('de_fora', 'N<+pp'), ('porque', 'SUB+conj-s'), (',', ','), ('se', 'SUB+conj-s'), ('as', '>N+art'), ('obras', 'H+n'), ('vierem', 'AUX+v-fin'), ('a', 'PRT-AUX<+prp'), ('ser', 'AUX+v-inf'), ('financiadas', 'MV+v-pcp'), (',', ','), ('eles', 'SUBJ+pron-pers'), ('até', 'ADVL+adv'), ('saem', 'P+v-fin'), ('de_graça', 'ADVS+pp'), (',', ','), ('já_que', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('esse', '>N+pron-det'), ('caso', 'H+n'), (',', ','), ('«', '«'), ('os', '>N+art'), ('fundos', 'H+n'), ('comunitários', 'N<+adj'), ('pagam', 'P+v-fin'), ('os', '>N+art'), ('projectos', 'H+n'), (',', ','), ('o', '>N+art'), ('mesmo', 'H+pron-det'), ('não', 'ADVL+adv'), ('acontecendo', 'P+v-ger'), ('quando', 'ADVL+adv'), ('eles', 'SUBJ+pron-pers'), ('são', 'AUX+v-fin'), ('feitos', 'MV+v-pcp'), ('por', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), (',', ','), ('dado', 'P+v-pcp'), ('serem', 'P+v-inf'), ('organismos', 'H+n'), ('de', 'H+prp'), ('o', '>N+art'), ('Estado', 'H+n'), ('.', '.')]\n",
      "--------\n",
      "[('Essa', 'SUBJ+pron-det'), ('poderá', 'AUX+v-fin'), ('vir', 'AUX+v-inf'), ('a', 'PRT-AUX<+prp'), ('ser', 'MV+v-inf'), ('uma', '>N+art'), ('hipótese', 'H+n'), (',', ','), ('até', '>S+adv'), ('porque', 'SUB+conj-s'), (',', ','), ('em', 'H+prp'), ('o', '>N+art'), ('terreno', 'H+n'), (',', ','), ('a', '>N+art'), ('capacidade', 'H+n'), ('de', 'H+prp'), ('os', '>N+art'), ('GAT', 'H+prop'), ('está', 'P+v-fin'), ('cada_vez_mais', '>A+adv'), ('enfraquecida', 'H+v-pcp'), ('.', '.')]\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "#Observando por sentenças\n",
    "tagged_sents = floresta.tagged_sents()\n",
    "\n",
    "for sent in tagged_sents[:10]:\n",
    "    print(sent)\n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando sentenças simplificadas\n",
    "simplified_sents = [[(n, simple_tag(t)) for n, t in sent] for sent in tagged_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Um', 'art'), ('revivalismo', 'n'), ('refrescante', 'adj')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(simplified_sents)\n",
    "\n",
    "num_sents = len(simplified_sents)\n",
    "len_train = int(num_sents*0.8)\n",
    "\n",
    "train = simplified_sents[:len_train]\n",
    "test = simplified_sents[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Treinando POS taggers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuracia default tagger 0.18786394306312967\n",
      "Acuracia unigram tagger 0.8811119327109714\n",
      "Acuracia bigram tagger 0.8945836029207875\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import accuracy\n",
    "\n",
    "tagger0 = nltk.DefaultTagger('n')\n",
    "\n",
    "print('Acuracia default tagger', tagger0.evaluate(test))\n",
    "\n",
    "tagger1 = nltk.UnigramTagger(train, backoff=tagger0)\n",
    "\n",
    "print('Acuracia unigram tagger', tagger1.evaluate(test))\n",
    "\n",
    "tagger2 = nltk.BigramTagger(train, backoff=tagger1)\n",
    "\n",
    "print('Acuracia bigram tagger', tagger2.evaluate(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testando</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eu', 'pron-pers'), ('sou', 'v-fin'), ('felipe', 'n'), ('navarro', 'n')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger2.tag(tokens('eu sou felipe navarro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Atividade: Treine um classificador sintatico com os dados do laboratorio NILC</h2>\n",
    "    \n",
    "<a href='http://www.nilc.icmc.usp.br/nilc/tools/nilctaggers.html'>Ver aqui</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dica, abrir o arquivo dessa forma:\n",
    "\n",
    "#raw = open(file_name, 'r').read().decode('latin-1').encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Chunkers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Contrução de uma base de dados para um Chunker em portugues</h3>\n",
    "\n",
    "<a href=\"https://www.linguateca.pt/primeiroHAREM/harem_coleccaodourada_en.html\">Fonte inicial</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc>\n",
      "<docid>harem-871-07800</docid>\n",
      "<genero>web</genero>\n",
      "<origem>pt</origem>\n",
      "<texto>\n",
      "<organizacao tipo=\"instituicao\" morf=\"f,s\">abraco</organizacao> pagina principal\n",
      "<organizacao tipo=\"instituicao\" morf=\"f,s\">associacao de apoio a pessoas com vih/sida</organizacao>\n",
      "a <organizacao tipo=\"instituicao\"\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "raw_xml = open('datasets/ColeccaoDouradaHAREM.txt', 'r', encoding='latin-1').read()\n",
    "raw_xml = raw_xml.replace('|', ' ')\n",
    "raw_xml = normalize_text(raw_xml)\n",
    "print(raw_xml[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num textos 129\n",
      "....\n",
      "Num textos com pessoas 78\n"
     ]
    }
   ],
   "source": [
    "soup = Soup(raw_xml)\n",
    "all_textos = soup.findAll('texto')\n",
    "num_textos = len(all_textos)\n",
    "print('Num textos', num_textos)\n",
    "print('....')\n",
    "texts_with_people = []\n",
    "\n",
    "for ctext in all_textos:\n",
    "    people = ctext.findAll('pessoa')\n",
    "    if len(people) > 1:\n",
    "        texts_with_people.append([ctext.text, [p.text for p in people]])\n",
    "\n",
    "print('Num textos com pessoas', len(texts_with_people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nfernando ferreira\\n[click for a page in english]\\ncmaf- universidade de lisboa gabinete a2-31 avenida professor gama pinto, 2 telefone do gabinete: 217904893 p-1649-003 lisboa extensao interna: 4293 portugal email: ferferr@cii.fc.ul.pt departamento de matematica faculdade de ciencias universidade de lisboa cmaf\\napresentacao\\nbem vindos aminha pagina pessoal. sou professor associado do departamento de matematica da universidade de lisboa e membro do centro de matematica e aplicacoes fundamentais - cmaf. clique aqui para obter o meu cv.\\ninteresses academicos\\nlogica matematica, em especial teorias fracas da aritmetica e da analise. filosofia  e fundamentos de matematica . tenho um interesse amador (no sentido latino da palavra) por alguns problemas da filosofia antiga , particularmente no problema da falsidade em parmenides e platao. tambem escrevi alguns ensaios expositorios sobre temas da logica: clique aqui para os ver.\\nensino\\nno presente semestre dou aulas teorico-praticas de algebra 2, cadeira do segundo ano das licenciaturas em matematica. o professor jose perdigao dias da silva eo regente da cadeira.\\nno semestre passado fui responsavel pelas cadeiras de topologia e introducao aanalise funcional, do terceiro ano das licenciaturas em matematica, e de teoria da demonstracao, do \\nmestrado em matematica.\\nno ano passado ensinei a cadeira de logica matematica aos finalistas de matematica e licenciaturas relacionadas. clique aqui para\\nver a pagina desta cadeira. tambem dei a cadeira logica de primeira-ordem ao primeiro ano das licenciaturas em informatica e engenharia da linguagem e do conhecimento. a pagina web desta cadeira ainda se encontra disponivel on-line em html://www.alf1.cii.fc.ul.pt/~ferferr/lpo.html .\\ntambem colaboro no mestrado em filosofia da linguagem e da consciencia da faculdade de letras.\\neventos\\nde 25 a 28 de junho decorrera em lisboa, no cmaf, a school on real algebraic and analytic geometry and o-minimal structures .\\nas quintas-feiras decorre o seminario de logica matematica (slm), organizado por mim e pelo professor narciso garcia do instituto superior tecnico. se quiser ter noticias regulares sobre o slm, por favor contacte-me.\\nvaria\\nsou co-editor da disputatio , uma revista de filosofia analitica.\\n',\n",
       " ['fernando ferreira',\n",
       "  'professor associado',\n",
       "  'professor jose perdigao dias da silva',\n",
       "  'professor narciso garcia']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_people[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fernando', 'B-nome'],\n",
       " ['ferreira', 'I-nome'],\n",
       " ['', 'O'],\n",
       " ['click', 'O'],\n",
       " ['for', 'O'],\n",
       " ['a', 'O'],\n",
       " ['page', 'O'],\n",
       " ['in', 'O'],\n",
       " ['english', 'O'],\n",
       " ['', 'O'],\n",
       " ['cmaf-', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['gabinete', 'O'],\n",
       " ['a2-31', 'O'],\n",
       " ['avenida', 'O'],\n",
       " ['professor', 'O'],\n",
       " ['gama', 'O'],\n",
       " ['pinto', 'O'],\n",
       " ['', 'O'],\n",
       " ['2', 'O'],\n",
       " ['telefone', 'O'],\n",
       " ['do', 'O'],\n",
       " ['gabinete', 'O'],\n",
       " ['', 'O'],\n",
       " ['217904893', 'O'],\n",
       " ['p-1649-003', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['extensao', 'O'],\n",
       " ['interna', 'O'],\n",
       " ['', 'O'],\n",
       " ['4293', 'O'],\n",
       " ['portugal', 'O'],\n",
       " ['email', 'O'],\n",
       " ['', 'O'],\n",
       " ['ferferr', 'O'],\n",
       " ['', 'O'],\n",
       " ['cii.fc.ul.pt', 'O'],\n",
       " ['departamento', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['faculdade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['ciencias', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['apresentacao', 'O'],\n",
       " ['bem', 'O'],\n",
       " ['vindos', 'O'],\n",
       " ['aminha', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['pessoal', 'O'],\n",
       " ['.', 'O'],\n",
       " ['sou', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['associado', 'I-nome'],\n",
       " ['do', 'O'],\n",
       " ['departamento', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['da', 'O'],\n",
       " ['universidade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['e', 'O'],\n",
       " ['membro', 'O'],\n",
       " ['do', 'O'],\n",
       " ['centro', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['aplicacoes', 'O'],\n",
       " ['fundamentais', 'O'],\n",
       " ['-', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['.', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['obter', 'O'],\n",
       " ['o', 'O'],\n",
       " ['meu', 'O'],\n",
       " ['cv', 'O'],\n",
       " ['.', 'O'],\n",
       " ['interesses', 'O'],\n",
       " ['academicos', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['em', 'O'],\n",
       " ['especial', 'O'],\n",
       " ['teorias', 'O'],\n",
       " ['fracas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['aritmetica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['da', 'O'],\n",
       " ['analise', 'O'],\n",
       " ['.', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['e', 'O'],\n",
       " ['fundamentos', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tenho', 'O'],\n",
       " ['um', 'O'],\n",
       " ['interesse', 'O'],\n",
       " ['amador', 'O'],\n",
       " ['', 'O'],\n",
       " ['no', 'O'],\n",
       " ['sentido', 'O'],\n",
       " ['latino', 'O'],\n",
       " ['da', 'O'],\n",
       " ['palavra', 'O'],\n",
       " ['', 'O'],\n",
       " ['por', 'O'],\n",
       " ['alguns', 'O'],\n",
       " ['problemas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['antiga', 'O'],\n",
       " ['', 'O'],\n",
       " ['particularmente', 'O'],\n",
       " ['no', 'O'],\n",
       " ['problema', 'O'],\n",
       " ['da', 'O'],\n",
       " ['falsidade', 'O'],\n",
       " ['em', 'O'],\n",
       " ['parmenides', 'O'],\n",
       " ['e', 'O'],\n",
       " ['platao', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['escrevi', 'O'],\n",
       " ['alguns', 'O'],\n",
       " ['ensaios', 'O'],\n",
       " ['expositorios', 'O'],\n",
       " ['sobre', 'O'],\n",
       " ['temas', 'O'],\n",
       " ['da', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['os', 'O'],\n",
       " ['ver', 'O'],\n",
       " ['.', 'O'],\n",
       " ['ensino', 'O'],\n",
       " ['no', 'O'],\n",
       " ['presente', 'O'],\n",
       " ['semestre', 'O'],\n",
       " ['dou', 'O'],\n",
       " ['aulas', 'O'],\n",
       " ['teorico-praticas', 'O'],\n",
       " ['de', 'O'],\n",
       " ['algebra', 'O'],\n",
       " ['2', 'O'],\n",
       " ['', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['do', 'O'],\n",
       " ['segundo', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['o', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['jose', 'I-nome'],\n",
       " ['perdigao', 'I-nome'],\n",
       " ['dias', 'I-nome'],\n",
       " ['da', 'I-nome'],\n",
       " ['silva', 'I-nome'],\n",
       " ['eo', 'O'],\n",
       " ['regente', 'O'],\n",
       " ['da', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['.', 'O'],\n",
       " ['no', 'O'],\n",
       " ['semestre', 'O'],\n",
       " ['passado', 'O'],\n",
       " ['fui', 'O'],\n",
       " ['responsavel', 'O'],\n",
       " ['pelas', 'O'],\n",
       " ['cadeiras', 'O'],\n",
       " ['de', 'O'],\n",
       " ['topologia', 'O'],\n",
       " ['e', 'O'],\n",
       " ['introducao', 'O'],\n",
       " ['aanalise', 'O'],\n",
       " ['funcional', 'O'],\n",
       " ['', 'O'],\n",
       " ['do', 'O'],\n",
       " ['terceiro', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['e', 'O'],\n",
       " ['de', 'O'],\n",
       " ['teoria', 'O'],\n",
       " ['da', 'O'],\n",
       " ['demonstracao', 'O'],\n",
       " ['', 'O'],\n",
       " ['do', 'O'],\n",
       " ['mestrado', 'O'],\n",
       " ['em', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['.', 'O'],\n",
       " ['no', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['passado', 'O'],\n",
       " ['ensinei', 'O'],\n",
       " ['a', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['de', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['aos', 'O'],\n",
       " ['finalistas', 'O'],\n",
       " ['de', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['relacionadas', 'O'],\n",
       " ['.', 'O'],\n",
       " ['clique', 'O'],\n",
       " ['aqui', 'O'],\n",
       " ['para', 'O'],\n",
       " ['ver', 'O'],\n",
       " ['a', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['desta', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['dei', 'O'],\n",
       " ['a', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['de', 'O'],\n",
       " ['primeira-ordem', 'O'],\n",
       " ['ao', 'O'],\n",
       " ['primeiro', 'O'],\n",
       " ['ano', 'O'],\n",
       " ['das', 'O'],\n",
       " ['licenciaturas', 'O'],\n",
       " ['em', 'O'],\n",
       " ['informatica', 'O'],\n",
       " ['e', 'O'],\n",
       " ['engenharia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['linguagem', 'O'],\n",
       " ['e', 'O'],\n",
       " ['do', 'O'],\n",
       " ['conhecimento', 'O'],\n",
       " ['.', 'O'],\n",
       " ['a', 'O'],\n",
       " ['pagina', 'O'],\n",
       " ['web', 'O'],\n",
       " ['desta', 'O'],\n",
       " ['cadeira', 'O'],\n",
       " ['ainda', 'O'],\n",
       " ['se', 'O'],\n",
       " ['encontra', 'O'],\n",
       " ['disponivel', 'O'],\n",
       " ['on-line', 'O'],\n",
       " ['em', 'O'],\n",
       " ['html', 'O'],\n",
       " ['', 'O'],\n",
       " ['//www.alf1.cii.fc.ul.pt/~ferferr/lpo.html', 'O'],\n",
       " ['.', 'O'],\n",
       " ['tambem', 'O'],\n",
       " ['colaboro', 'O'],\n",
       " ['no', 'O'],\n",
       " ['mestrado', 'O'],\n",
       " ['em', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['linguagem', 'O'],\n",
       " ['e', 'O'],\n",
       " ['da', 'O'],\n",
       " ['consciencia', 'O'],\n",
       " ['da', 'O'],\n",
       " ['faculdade', 'O'],\n",
       " ['de', 'O'],\n",
       " ['letras', 'O'],\n",
       " ['.', 'O'],\n",
       " ['eventos', 'O'],\n",
       " ['de', 'O'],\n",
       " ['25', 'O'],\n",
       " ['a', 'O'],\n",
       " ['28', 'O'],\n",
       " ['de', 'O'],\n",
       " ['junho', 'O'],\n",
       " ['decorrera', 'O'],\n",
       " ['em', 'O'],\n",
       " ['lisboa', 'O'],\n",
       " ['', 'O'],\n",
       " ['no', 'O'],\n",
       " ['cmaf', 'O'],\n",
       " ['', 'O'],\n",
       " ['a', 'O'],\n",
       " ['school', 'O'],\n",
       " ['on', 'O'],\n",
       " ['real', 'O'],\n",
       " ['algebraic', 'O'],\n",
       " ['and', 'O'],\n",
       " ['analytic', 'O'],\n",
       " ['geometry', 'O'],\n",
       " ['and', 'O'],\n",
       " ['o-minimal', 'O'],\n",
       " ['structures', 'O'],\n",
       " ['.', 'O'],\n",
       " ['as', 'O'],\n",
       " ['quintas-feiras', 'O'],\n",
       " ['decorre', 'O'],\n",
       " ['o', 'O'],\n",
       " ['seminario', 'O'],\n",
       " ['de', 'O'],\n",
       " ['logica', 'O'],\n",
       " ['matematica', 'O'],\n",
       " ['', 'O'],\n",
       " ['slm', 'O'],\n",
       " ['', 'O'],\n",
       " ['', 'O'],\n",
       " ['organizado', 'O'],\n",
       " ['por', 'O'],\n",
       " ['mim', 'O'],\n",
       " ['e', 'O'],\n",
       " ['pelo', 'O'],\n",
       " ['professor', 'B-nome'],\n",
       " ['narciso', 'I-nome'],\n",
       " ['garcia', 'I-nome'],\n",
       " ['do', 'O'],\n",
       " ['instituto', 'O'],\n",
       " ['superior', 'O'],\n",
       " ['tecnico', 'O'],\n",
       " ['.', 'O'],\n",
       " ['se', 'O'],\n",
       " ['quiser', 'O'],\n",
       " ['ter', 'O'],\n",
       " ['noticias', 'O'],\n",
       " ['regulares', 'O'],\n",
       " ['sobre', 'O'],\n",
       " ['o', 'O'],\n",
       " ['slm', 'O'],\n",
       " ['', 'O'],\n",
       " ['por', 'O'],\n",
       " ['favor', 'O'],\n",
       " ['contacte-me', 'O'],\n",
       " ['.', 'O'],\n",
       " ['varia', 'O'],\n",
       " ['sou', 'O'],\n",
       " ['co-editor', 'O'],\n",
       " ['da', 'O'],\n",
       " ['disputatio', 'O'],\n",
       " ['', 'O'],\n",
       " ['uma', 'O'],\n",
       " ['revista', 'O'],\n",
       " ['de', 'O'],\n",
       " ['filosofia', 'O'],\n",
       " ['analitica', 'O'],\n",
       " ['.', 'O']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "def create_tagged_people_names(name, tag_name):\n",
    "    tokens = nltk.word_tokenize(name)\n",
    "    first_token = tokens[0] +'|B-' +tag_name\n",
    "    \n",
    "    middle_tokens = tokens[1:]\n",
    "    return ' '.join([first_token] + [t+'|I-'+tag_name for t in middle_tokens]) + ' '\n",
    "\n",
    "create_tagged_people_names('fernando ferreira', 'nome')\n",
    "\n",
    "def create_fully_tagged_text(full_text, names, tag_name):\n",
    "    tagged_names = [create_tagged_people_names(n, tag_name) for n in names]\n",
    "    modified_text = full_text\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        modified_text = modified_text.replace(name, tagged_names[i])\n",
    "        \n",
    "    tokens = nltk.word_tokenize(modified_text)\n",
    "    modified_tokens = []\n",
    "    for t in tokens:\n",
    "        if '|' in t:\n",
    "            modified_tokens.append(t)\n",
    "        else:\n",
    "            modified_tokens.append(t + '|O')\n",
    "    return ' '.join(modified_tokens)\n",
    "\n",
    "def create_separated_tag_text(tagged_text):\n",
    "    tokens = nltk.word_tokenize(tagged_text)\n",
    "    return [x.split('|') for x in tokens if len(x.split('|')) > 1]\n",
    "\n",
    "def process_entry(entry, tag_name):\n",
    "    text = entry[0]\n",
    "    names = entry[1]\n",
    "    ftt = create_fully_tagged_text(text, names, tag_name)\n",
    "    return create_separated_tag_text(ftt)\n",
    "\n",
    "process_entry(texts_with_people[0], 'nome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_entries = [process_entry(t, 'nome') for t in texts_with_people]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_for_words(sentence, index):\n",
    "    current_word = sentence[index]\n",
    "    prefix3 = current_word[:3]\n",
    "    sufix3 = current_word[-3:]\n",
    "    if index == 0:\n",
    "        prev_word = ''\n",
    "    else:\n",
    "        prev_word = sentence[index - 1]\n",
    "    feats = {'word': current_word,\n",
    "            'prefix3': prefix3,\n",
    "            'sufix3': sufix3,\n",
    "            'prev_word': prev_word}\n",
    "    return feats\n",
    "\n",
    "def processed_entry_to_feats_and_targets(processed_entry):\n",
    "    sentence = [w[0] for w in processed_entry]\n",
    "    feats_and_targets = [[create_features_for_words(sentence, i), w[1]] for i, w in enumerate(processed_entry)]\n",
    "    return feats_and_targets\n",
    "\n",
    "new_processed = []\n",
    "for pe in processed_entries:\n",
    "    new_processed += processed_entry_to_feats_and_targets(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feats = [np[0] for np in new_processed]\n",
    "Targets = [np[1] for np in new_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-nome', 'I-nome', 'O'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(Targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dataset 74211\n"
     ]
    }
   ],
   "source": [
    "Feats = [np[0] for np in new_processed]\n",
    "Targets = [np[1] for np in new_processed]\n",
    "\n",
    "print('Len dataset', len(new_processed))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "F_train, F_test, T_train, T_test = train_test_split(Feats, Targets, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Targets,\n",
    "                                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_target(target):\n",
    "    if target == 'O':\n",
    "        return 0\n",
    "    elif target == 'B-nome':\n",
    "        return 1\n",
    "    elif target == 'I-nome':\n",
    "        return 2\n",
    "    \n",
    "def convert_target_index(target_index):\n",
    "    if target_index == 0:\n",
    "        return 'O'\n",
    "    elif target_index == 1:\n",
    "        return 'B-nome'\n",
    "    elif target_index == 2:\n",
    "        return 'I-nome'\n",
    "\n",
    "T_train = [convert_target(t) for t in T_train]\n",
    "T_test = [convert_target(t) for t in T_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform(F_train)\n",
    "X_test = vectorizer.transform(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, T_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusao [[20708   222    97]\n",
      " [  156   785    16]\n",
      " [  125    19   136]]\n",
      "Precisao [0.98661203 0.76510721 0.54618474]\n",
      "Recall [0.98482903 0.82027168 0.48571429]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(T_test, prediction)\n",
    "print('Matriz de confusao', cm)\n",
    "\n",
    "precision = precision_score(T_test, prediction, average=None)\n",
    "recall = recall_score(T_test, prediction, average=None)\n",
    "\n",
    "print('Precisao', precision)\n",
    "print('Recall', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garantindo o bom comportamento do modelo em termos de notacao IOB\n",
    "\n",
    "def create_features_for_words_sl(sentence, index, previous_classification):\n",
    "    current_word = sentence[index]\n",
    "    prefix3 = current_word[:3]\n",
    "    sufix3 = current_word[-3:]\n",
    "    if index == 0:\n",
    "        prev_word = ''\n",
    "    else:\n",
    "        prev_word = sentence[index - 1]\n",
    "    feats = {'word': current_word,\n",
    "            'prefix3': prefix3,\n",
    "            'sufix3': sufix3,\n",
    "            'prev_word': prev_word,\n",
    "            'previous_classification': previous_classification}\n",
    "    return feats\n",
    "\n",
    "def processed_entry_to_feats_and_targets_sl(processed_entry):\n",
    "    sentence = [w[0] for w in processed_entry]\n",
    "    tags = [w[1] for w in processed_entry]\n",
    "    \n",
    "    previous_tag = 'O'\n",
    "    feats_and_targets = []\n",
    "    for i, w in enumerate(processed_entry):\n",
    "        feats_and_targets.append(create_features_for_words_sl(sentence, i, previous_tag))\n",
    "        previous_tag = tags[i]\n",
    "    return feats_and_targets\n",
    "\n",
    "new_processed_sl = []\n",
    "for pe in processed_entries:\n",
    "    new_processed_sl += processed_entry_to_feats_and_targets_sl(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusao [[20677   278    72]\n",
      " [  130   810    17]\n",
      " [  131    14   135]]\n",
      "Precisao [0.98753463 0.73502722 0.60267857]\n",
      "Recall [0.98335473 0.84639498 0.48214286]\n"
     ]
    }
   ],
   "source": [
    "Feats_sl = [np[0] for np in new_processed]\n",
    "Targets_sl = [np[1] for np in new_processed]\n",
    "\n",
    "F_train_sl, F_test_sl, T_train_sl, T_test_sl = train_test_split(Feats_sl, Targets_sl, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Targets_sl,\n",
    "                                                                    shuffle=True)\n",
    "\n",
    "\n",
    "T_train_sl = [convert_target(t) for t in T_train_sl]\n",
    "T_test_sl = [convert_target(t) for t in T_test_sl]\n",
    "\n",
    "vectorizer_sl = DictVectorizer()\n",
    "X_train_sl = vectorizer_sl.fit_transform(F_train_sl)\n",
    "X_test_sl = vectorizer_sl.transform(F_test_sl)\n",
    "\n",
    "model_sl = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model_sl.fit(X_train_sl, T_train_sl)\n",
    "\n",
    "prediction_sl = model_sl.predict(X_test_sl)\n",
    "\n",
    "cm_sl = confusion_matrix(T_test_sl, prediction_sl)\n",
    "print('Matriz de confusao', cm_sl)\n",
    "\n",
    "precision_sl = precision_score(T_test_sl, prediction_sl, average=None)\n",
    "recall_sl = recall_score(T_test_sl, prediction_sl, average=None)\n",
    "\n",
    "print('Precisao', precision_sl)\n",
    "print('Recall', recall_sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction_sl(text, classifier, vectorizer):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    last_prediction = 'O'\n",
    "    predictions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        feats = create_features_for_words_sl(tokens, i, last_prediction)\n",
    "        vect_feats = vectorizer.transform([feats])\n",
    "        prediction = convert_target_index(classifier.predict(vect_feats)[0])\n",
    "        last_prediction = prediction\n",
    "        predictions.append((token, prediction))\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('o', 'O'), ('meu', 'O'), ('nome', 'O'), ('eh', 'O'), ('fernando', 'O'), ('carlos', 'B-nome'), ('da', 'I-nome'), ('silva', 'I-nome'), (',', 'O'), ('entendeu', 'O'), ('?', 'O')]\n"
     ]
    }
   ],
   "source": [
    "prediction = make_prediction_sl('o meu nome eh fernando carlos da silva, entendeu?', model_sl, vectorizer_sl)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atividade: adicione outras features ao chunker, como a classificacao sintatica</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atividade: Crie outro chunker para outra tag desta base de dados</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exemplo de Análise de semântica latente</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import floresta\n",
    "all_sents = [' '.join(s) for s in floresta.sents()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9266"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um revivalismo refrescante',\n",
       " 'O 7_e_Meio é um ex-libris de a noite algarvia .',\n",
       " 'É uma de as mais antigas discotecas de o Algarve , situada em Albufeira , que continua a manter os traços decorativos e as clientelas de sempre .']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "nlp = spacy.load('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def lemmatize(sent):\n",
    "    scapy_sent = nlp(normalize_text(sent))\n",
    "    return ' '.join([token.lemma_ for token in scapy_sent])\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    tokens = [t for t in tokens if not any([c.isdigit() for c in t])]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['achar', 'encontrar', 'ontem', 'shopping']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(lemmatize('Acho que te encontrei ontem no shopping test2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9266/9266 [02:05<00:00, 73.76it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "transformed_sents = []\n",
    "for sent in tqdm(all_sents):\n",
    "    transformed_sents.append(' '.join(word_tokenize(lemmatize(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['revivalismo refrescante',\n",
       " 'libris noite algarvio',\n",
       " 'umar antigo discoteca algarve situar albufeirar continuar manter tracos decorativo clientela sempre',\n",
       " 'um_pouco versao umar especie outro lado noite mear caminhar entrar devaneio umar fauna periferica ser lisboa londres dublin farar portimao posturar circunspecto fiar casar esperar musicar geracionista',\n",
       " 'nao deixar ser tempo correr certar very_typical algarvio cabeca cartaz parir querer fugir algum movimentacoes nocturno caminhar ritualizacao massa genero todo calypso encontramos- locomia',\n",
       " 'assim milhoes ministerio_do_planeamento_e_administracao_do_territorio gasto pagamento pessoal afectar organismo vir juntar- montante obrar propriamente dizer municipios projecto mao vir reivindicar juntar executivo comer salientar membro governar',\n",
       " 'dinheiro nao falto camaras lembrar secretario_de_estado considerar solucao parir autarquia especializarem- fundo comunitarios',\n",
       " 'comer nao dispoem quadro tecnicos necessarios',\n",
       " 'encomendem- projectista de_fora porque obrar vir ser financiar atar sair de_graca ja_que casar fundo comunitarios pagar projecto nao acontecer sao feito gat dar ser organismo estar',\n",
       " 'podera ver ser umar hipotese atar porque terreno capacidade gat cada_vez_mais enfraquecido']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term_m = vectorizer.fit_transform(transformed_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9266x18798 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 101793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_term_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_document_term_m = pd.DataFrame(document_term_m.todense(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__the_new_york_times</th>\n",
       "      <th>_a_luz_do_verbo</th>\n",
       "      <th>_a_melhor_opcao_de_investimento</th>\n",
       "      <th>_a_obra_e_o_homem</th>\n",
       "      <th>_a_pergunta_que_nao_quer_calar</th>\n",
       "      <th>_acores</th>\n",
       "      <th>_agricultura_</th>\n",
       "      <th>_agricultura_e_a_solucao</th>\n",
       "      <th>_allatri</th>\n",
       "      <th>_and_grew_up_with_the_country</th>\n",
       "      <th>...</th>\n",
       "      <th>zonear</th>\n",
       "      <th>zonzo</th>\n",
       "      <th>zoologico</th>\n",
       "      <th>zoomp</th>\n",
       "      <th>zootv</th>\n",
       "      <th>zubizarreta</th>\n",
       "      <th>zucco</th>\n",
       "      <th>zuelle</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zurique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18798 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   __the_new_york_times  _a_luz_do_verbo  _a_melhor_opcao_de_investimento  \\\n",
       "0                   0.0              0.0                              0.0   \n",
       "1                   0.0              0.0                              0.0   \n",
       "2                   0.0              0.0                              0.0   \n",
       "3                   0.0              0.0                              0.0   \n",
       "4                   0.0              0.0                              0.0   \n",
       "\n",
       "   _a_obra_e_o_homem  _a_pergunta_que_nao_quer_calar  _acores  _agricultura_  \\\n",
       "0                0.0                             0.0      0.0            0.0   \n",
       "1                0.0                             0.0      0.0            0.0   \n",
       "2                0.0                             0.0      0.0            0.0   \n",
       "3                0.0                             0.0      0.0            0.0   \n",
       "4                0.0                             0.0      0.0            0.0   \n",
       "\n",
       "   _agricultura_e_a_solucao  _allatri  _and_grew_up_with_the_country   ...     \\\n",
       "0                       0.0       0.0                            0.0   ...      \n",
       "1                       0.0       0.0                            0.0   ...      \n",
       "2                       0.0       0.0                            0.0   ...      \n",
       "3                       0.0       0.0                            0.0   ...      \n",
       "4                       0.0       0.0                            0.0   ...      \n",
       "\n",
       "   zonear  zonzo  zoologico  zoomp  zootv  zubizarreta  zucco  zuelle  zulu  \\\n",
       "0     0.0    0.0        0.0    0.0    0.0          0.0    0.0     0.0   0.0   \n",
       "1     0.0    0.0        0.0    0.0    0.0          0.0    0.0     0.0   0.0   \n",
       "2     0.0    0.0        0.0    0.0    0.0          0.0    0.0     0.0   0.0   \n",
       "3     0.0    0.0        0.0    0.0    0.0          0.0    0.0     0.0   0.0   \n",
       "4     0.0    0.0        0.0    0.0    0.0          0.0    0.0     0.0   0.0   \n",
       "\n",
       "   zurique  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "\n",
       "[5 rows x 18798 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_term_m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_term_document = df_document_term_m.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>9256</th>\n",
       "      <th>9257</th>\n",
       "      <th>9258</th>\n",
       "      <th>9259</th>\n",
       "      <th>9260</th>\n",
       "      <th>9261</th>\n",
       "      <th>9262</th>\n",
       "      <th>9263</th>\n",
       "      <th>9264</th>\n",
       "      <th>9265</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>__the_new_york_times</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_a_luz_do_verbo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_a_melhor_opcao_de_investimento</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_a_obra_e_o_homem</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_a_pergunta_que_nao_quer_calar</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9266 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0     1     2     3     4     5     6     \\\n",
       "__the_new_york_times              0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_a_luz_do_verbo                   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_a_melhor_opcao_de_investimento   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_a_obra_e_o_homem                 0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "_a_pergunta_que_nao_quer_calar    0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "                                 7     8     9     ...   9256  9257  9258  \\\n",
       "__the_new_york_times              0.0   0.0   0.0  ...    0.0   0.0   0.0   \n",
       "_a_luz_do_verbo                   0.0   0.0   0.0  ...    0.0   0.0   0.0   \n",
       "_a_melhor_opcao_de_investimento   0.0   0.0   0.0  ...    0.0   0.0   0.0   \n",
       "_a_obra_e_o_homem                 0.0   0.0   0.0  ...    0.0   0.0   0.0   \n",
       "_a_pergunta_que_nao_quer_calar    0.0   0.0   0.0  ...    0.0   0.0   0.0   \n",
       "\n",
       "                                 9259  9260  9261  9262  9263  9264  9265  \n",
       "__the_new_york_times              0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "_a_luz_do_verbo                   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "_a_melhor_opcao_de_investimento   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "_a_obra_e_o_homem                 0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "_a_pergunta_que_nao_quer_calar    0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 9266 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_term_document.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(2, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(df_term_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00227087, -0.000826  ],\n",
       "       [ 0.0020832 , -0.00207978],\n",
       "       [ 0.00068387, -0.00047862],\n",
       "       ...,\n",
       "       [ 0.00290644, -0.00245849],\n",
       "       [ 0.0030136 , -0.00214634],\n",
       "       [ 0.00157841, -0.00189297]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(dtm_lsa[:,0], dtm_lsa[:, 1])\n",
    "for i, word in enumerate(vectorizer.get_feature_names()):\n",
    "    plt.annotate(s = word, xy=dtm_lsa[i,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
