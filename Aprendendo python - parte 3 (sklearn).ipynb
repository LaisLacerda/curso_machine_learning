{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sklearn</h1>\n",
    "<br>\n",
    "O sklearn é uma das principais bibliotecas para aprendizagem de máquinas em Python. Os algorítmos mais populares possuem implementações nessa biblioteca, com excessão de técnicas que envolvam <i>Deep Learning</i>. Para tal, utilize bibliotecas como o <b>Tensorflow</b> ou <b>Pytorch</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Base de dados de teste</h2>\n",
    "<br>\n",
    "Para fins de teste, usaremos alguns conjuntos de dados teste que já estão disponíveis no sklearn. Inicialmente usaremos o iris dataset, que contém dados para a classificação de flores.\n",
    "<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/1920px-Iris_versicolor_3.jpg\" height=\"500\" width=\"500\">\n",
    "<i>Fonte da imagem: Wikipedia: https://upload.wikimedia.org/wikipedia/commons/thumb/4/41/Iris_versicolor_3.jpg/1920px-Iris_versicolor_3.jpg</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE X (150, 4)\n",
      "SHAPE Y (150,)\n",
      "TAG SET {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "print('SHAPE X', np.shape(X))\n",
    "print('SHAPE Y', np.shape(Y))\n",
    "\n",
    "tag_set = set(Y)\n",
    "print('TAG SET', tag_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Criando conjuntos de treinamento, teste e validação</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE X_train_test (105, 4)\n",
      "SHAPE X_test (45, 4)\n",
      "SET Y_train {0, 1, 2}\n",
      "SET Y_test {0, 1, 2}\n",
      "SHAPE train (84, 4)\n",
      "SHAPE validation (21, 4)\n",
      "SHAPE test (45, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train_initial, X_test, Y_train_initial, Y_test = train_test_split(X, Y, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Y,\n",
    "                                                                    shuffle=True)\n",
    "\n",
    "print('SHAPE X_train_test', np.shape(X_train_initial))\n",
    "print('SHAPE X_test', np.shape(X_test))\n",
    "print('SET Y_train', set(Y_train_initial))\n",
    "print('SET Y_test', set(Y_test))\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train_initial, Y_train_initial, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=Y_train_initial,\n",
    "                                                                shuffle=True)\n",
    "\n",
    "print('SHAPE train', np.shape(X_train))\n",
    "print('SHAPE validation', np.shape(X_validation))\n",
    "print('SHAPE test', np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Testando classificadores no conjunto de validação e no conjunto de teste</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB - acuracia no conjunto de validacao 0.9047619047619048\n",
      "BernoulliNB - acuracia no conjunto de validacao 0.3333333333333333\n",
      "GaussianNB - acuracia no conjunto de teste 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model_1 = GaussianNB()\n",
    "model_1.fit(X_train, Y_train)\n",
    "\n",
    "model_2 = BernoulliNB()\n",
    "model_2.fit(X_train, Y_train)\n",
    "\n",
    "accuracy_validation_1 = model_1.score(X_validation, Y_validation)\n",
    "accuracy_validation_2 = model_2.score(X_validation, Y_validation)\n",
    "\n",
    "print('GaussianNB - acuracia no conjunto de validacao', accuracy_validation_1)\n",
    "print('BernoulliNB - acuracia no conjunto de validacao', accuracy_validation_2)\n",
    "\n",
    "#Para testar o segundo classificador\n",
    "#accuracy_validation_2 = 1.0\n",
    "\n",
    "if accuracy_validation_1 > accuracy_validation_2:\n",
    "    accuracy_test = model_1.score(X_test, Y_test)\n",
    "    print('GaussianNB - acuracia no conjunto de teste', accuracy_test)\n",
    "else:\n",
    "    accuracy_test = model_2.score(X_test, Y_test)\n",
    "    print('BernoulliNB - acuracia no conjunto de teste', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rodando o classificador em um conjunto de exemplos e salvando o modelo</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [2 0 2 2 2 0 1 1 0 2] vs Target: [2 0 2 2 2 0 1 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "#Realizando a previsao em um conjunto de dados\n",
    "\n",
    "prediction = model_1.predict(X_test)\n",
    "print('Prediction:', prediction[:10], 'vs Target:', Y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [2 0 2 2 2 0 1 1 0 2] vs Target: [2 0 2 2 2 0 1 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "#Salvando o modelo\n",
    "\n",
    "#Existem duas formas diferentes\n",
    "\n",
    "#Salvando....\n",
    "import pickle\n",
    "out = open('model_1.pickle', 'wb')\n",
    "pickle.dump(model_1, out)\n",
    "out.close()\n",
    "\n",
    "#Carregando\n",
    "model_saved = pickle.load(open('model_1.pickle', 'rb'))\n",
    "prediction = model_saved.predict(X_test)\n",
    "print('Prediction:', prediction[:10], 'vs Target:', Y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [2 0 2 2 2 0 1 1 0 2] vs Target: [2 0 2 2 2 0 1 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "#Segunda forma\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Salvando\n",
    "joblib.dump(model_1, 'model_1.pkl')\n",
    "\n",
    "#Carregando\n",
    "model_saved2 = joblib.load('model_1.pkl')\n",
    "prediction = model_saved2.predict(X_test)\n",
    "print('Prediction:', prediction[:10], 'vs Target:', Y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculando as demais métricas dos classificadores</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisao do GaussianNB por classe [1.         0.88235294 1.        ]\n",
      "Precisao do GaussianNB na media 0.9607843137254902\n",
      "Precisao do BernoulliNB por classe [0.33333333 0.         0.        ]\n",
      "Precisao do BernoulliNB na media 0.1111111111111111\n",
      "Recall do GaussianNB por classe [1.         1.         0.86666667]\n",
      "Recall do GaussianNB na media 0.9555555555555556\n",
      "Recall do BernoulliNB por classe [1. 0. 0.]\n",
      "Recall do BernoulliNB na media 0.3333333333333333\n",
      "Matriz de confusao - GaussianNB\n",
      " [[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  2 13]]\n",
      "Matriz de confusao - BernoulliNB\n",
      " [[15  0  0]\n",
      " [15  0  0]\n",
      " [15  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fnba/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "Y_pred_1 = model_1.predict(X_test)\n",
    "Y_pred_2 = model_2.predict(X_test)\n",
    "\n",
    "precision_1 = precision_score(Y_test, Y_pred_1, average=None)\n",
    "precision_1_average = precision_score(Y_test, Y_pred_1, average='weighted')\n",
    "print('Precisao do GaussianNB por classe', precision_1)\n",
    "print('Precisao do GaussianNB na media', precision_1_average)\n",
    "\n",
    "precision_2 = precision_score(Y_test, Y_pred_2, average=None)\n",
    "precision_2_average = precision_score(Y_test, Y_pred_2, average='weighted')\n",
    "print('Precisao do BernoulliNB por classe', precision_2)\n",
    "print('Precisao do BernoulliNB na media', precision_2_average)\n",
    "\n",
    "recall_1 = recall_score(Y_test, Y_pred_1, average=None)\n",
    "recall_1_average = recall_score(Y_test, Y_pred_1, average='weighted')\n",
    "print('Recall do GaussianNB por classe', recall_1)\n",
    "print('Recall do GaussianNB na media', recall_1_average)\n",
    "\n",
    "recall_2 = recall_score(Y_test, Y_pred_2, average=None)\n",
    "recall_2_average = recall_score(Y_test, Y_pred_2, average='weighted')\n",
    "print('Recall do BernoulliNB por classe', recall_2)\n",
    "print('Recall do BernoulliNB na media', recall_2_average)\n",
    "\n",
    "cm_1 = confusion_matrix(Y_test, Y_pred_1)\n",
    "print('Matriz de confusao - GaussianNB\\n', cm_1)\n",
    "\n",
    "cm_2 = confusion_matrix(Y_test, Y_pred_2)\n",
    "print('Matriz de confusao - BernoulliNB\\n', cm_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atividade: Teste diversos classificadores utilizando o digits dataset</h1>\n",
    "<br>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\" height=\"500\" width=\"500\">\n",
    "<i>Fonte da imagem: Wikipedia: https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png</i>\n",
    "<br>\n",
    "<h4>Pesquisar classificadores aqui:</h4>\n",
    "<a href=\"https://scikit-learn.org/stable/supervised_learning.html\" target=\"_blank\">Referência do sklearn</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE X (1797, 64)\n",
      "SHAPE Y (1797,)\n",
      "TAG SET {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_digits()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "print('SHAPE X', np.shape(X))\n",
    "print('SHAPE Y', np.shape(Y))\n",
    "\n",
    "tag_set = set(Y)\n",
    "print('TAG SET', tag_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Escalabilidade no treinamento - minibatches</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#Operador yield\n",
    "\n",
    "def create_squares_1(my_list):\n",
    "    squared_list = []\n",
    "    for elem in my_list:\n",
    "        squared_list.append(elem ** 2)\n",
    "    return squared_list\n",
    "\n",
    "squared_list = create_squares_1([1,2,3,4])\n",
    "\n",
    "for elem in squared_list:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#Alternativa\n",
    "\n",
    "def create_squares_1(my_list):\n",
    "    squared_list = []\n",
    "    for elem in my_list:\n",
    "        yield elem ** 2\n",
    "\n",
    "squared_list_gen = create_squares_1([1,2,3,4])\n",
    "\n",
    "for elem in squared_list:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exemplo de treinamento em minibatch com o digits dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregando o digits dataset\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "iris = datasets.load_digits()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "X_train_initial, X_test, Y_train_initial, Y_test = train_test_split(X, Y, \n",
    "                                                                    test_size=0.30, \n",
    "                                                                    stratify=Y,\n",
    "                                                                    shuffle=True)\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X_train_initial, Y_train_initial, \n",
    "                                                                test_size=0.2, \n",
    "                                                                stratify=Y_train_initial,\n",
    "                                                                shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 3\n",
      "Epoch 1 of 3\n",
      "Epoch 2 of 3\n",
      "Accuracy 0.9047619047619048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felipe\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Rodar mais de uma vez\n",
    "\n",
    "def create_batches(X_set, Y_set, size_batch):\n",
    "    len_X = np.shape(X_set)[0]\n",
    "    num_batches = int(math.floor(len_X/size_batch))\n",
    "    start = 0\n",
    "    for i in range(num_batches):\n",
    "        x_batch = X_set[start:start + size_batch]\n",
    "        y_batch = Y_set[start:start + size_batch]\n",
    "        start += size_batch\n",
    "        yield x_batch, y_batch\n",
    "\n",
    "model = SGDClassifier(loss='hinge')\n",
    "num_epochs = 3\n",
    "\n",
    "my_classes = (0,1,2,3,4,5,6,7,8,9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch', epoch, 'of', num_epochs)\n",
    "    batch_generator = create_batches(X_train, Y_train, 30)\n",
    "    \n",
    "    for batch_x, batch_y in batch_generator:\n",
    "        model.partial_fit(batch_x, batch_y, classes=my_classes)\n",
    "\n",
    "accuracy_validation = model.score(X_validation, Y_validation)\n",
    "\n",
    "print('Accuracy', accuracy_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Exemplo de classificação de texto com Sklearn</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "raw_file = open('datasets/reviews.json', 'r').read()\n",
    "as_json = json.loads(raw_file)\n",
    "num_texts = len(as_json['paper'])\n",
    "\n",
    "print(num_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'preliminary_decision': 'accept',\n",
       " 'review': [{'confidence': '4',\n",
       "   'evaluation': '1',\n",
       "   'id': 1,\n",
       "   'lan': 'es',\n",
       "   'orientation': '0',\n",
       "   'remarks': '',\n",
       "   'text': '- El artículo aborda un problema contingente y muy relevante, e incluye tanto un diagnóstico nacional de uso de buenas prácticas como una solución (buenas prácticas concretas). - El lenguaje es adecuado.  - El artículo se siente como la concatenación de tres artículos diferentes: (1) resultados de una encuesta, (2) buenas prácticas de seguridad, (3) incorporación de buenas prácticas. - El orden de las secciones sería mejor si refleja este orden (la versión revisada es #2, #1, #3). - El artículo no tiene validación de ningún tipo, ni siquiera por evaluación de expertos.',\n",
       "   'timespan': '2010-07-05'},\n",
       "  {'confidence': '4',\n",
       "   'evaluation': '1',\n",
       "   'id': 2,\n",
       "   'lan': 'es',\n",
       "   'orientation': '1',\n",
       "   'remarks': '',\n",
       "   'text': 'El artículo presenta recomendaciones prácticas para el desarrollo de software seguro. Se describen las mejores prácticas recomendadas para desarrollar software que sea proactivo ante los ataques, y se realiza un análisis de costos de estas prácticas en desarrollo de software. Todo basado en una revisión de prácticas propuestas en la bibliografía y su contraste con datos obtenidos de una encuesta en empresas. Finalmente se recomienda una guía.  Sería ideal aplicar la guía propuesta a empresas no involucradas en la encuesta que sirvió para originarla de modo de poder evaluar su efectividad en forma independiente.',\n",
       "   'timespan': '2010-07-05'},\n",
       "  {'confidence': '5',\n",
       "   'evaluation': '1',\n",
       "   'id': 3,\n",
       "   'lan': 'es',\n",
       "   'orientation': '1',\n",
       "   'remarks': '',\n",
       "   'text': '- El tema es muy interesante y puede ser de mucha ayuda una guía para incorporar prácticas de seguridad. - La presentación (descripción, etapa y uso) de las 9 prácticas para el desarrollo de software seguro.  - El “estado real del desarrollo de software en Chile” (como lo indica en su paper) no se puede lograr con solamente 22 encuestas de un total de 50. - Presenta nueve tablas que corresponden a las prácticas para el desarrollo de software seguro, pero la guía presenta 10 prácticas. ¿explica por qué? - Sugiero mejorar la guía, el mayor aporte está en la secuencia de incorporación que propone.  Además, no debería explicar la práctica en Observaciones ni diferenciarla con otras prácticas en esa columna, sino que debería dar sugerencias de cómo aplicarla. - En el texto indica “Más adelante, se presentan además tres prácticas extras…” ¿cuáles son o no leí correctamente? - De acuerdo a formato, poner como mínimo 5 palabras clave. - Sugiero mencionar las prácticas antes de mostrar cada tabla. - Algunas referencias están incompletas, por ejemplo, falta año en referencia 17, falta año y tipo de evento en referencia 11, falta editorial en referencia 19 (¿es un libro?) - Algunos títulos llevan una coma dentro de las comillas, ejemplo, referencia 1',\n",
       "   'timespan': '2010-07-05'}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observando os dados\n",
    "\n",
    "entries = [j for j in as_json['paper']]\n",
    "entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SET classifications ['accept', 'no decision', 'reject', 'probably reject']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = [' '.join([x['text'] for x in j['review']]) for j in entries]\n",
    "classifications = [j['preliminary_decision'] for j in entries]\n",
    "\n",
    "class_set = list(set(classifications))\n",
    "print('SET classifications', class_set)\n",
    "\n",
    "numeric_classifications = [class_set.index(c) for c in classifications]\n",
    "\n",
    "Y = np.array(numeric_classifications).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Criação de features numéricas</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_1 = CountVectorizer()\n",
    "vectorizer_2 = TfidfVectorizer()\n",
    "\n",
    "X_1 = vectorizer_1.fit_transform(texts)\n",
    "X_2 = vectorizer_2.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6704 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 192 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_1[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Teste dos classificadores</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fnba/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy GaussianNB + CountVectorizer 0.5384615384615384\n",
      "Accuracy GaussianNB + Tfidf 0.5192307692307693\n",
      "Accuracy LogisticRegression + CountVectorizer 0.7115384615384616\n",
      "Accuracy LogisticRegression + Tfidf 0.6730769230769231\n",
      "Accuracy MLPClassifier + CountVectorizer 0.6730769230769231\n",
      "Accuracy MLPClassifier + Tfidf 0.6730769230769231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = train_test_split(X_1, Y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    stratify=Y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train_2, X_test_2, Y_train_2, Y_test_2 = train_test_split(X_2, Y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    stratify=Y,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "model1_1 = GaussianNB()\n",
    "model1_2 = GaussianNB()\n",
    "model2_1 = LogisticRegression()\n",
    "model2_2 = LogisticRegression()\n",
    "model3_1 = MLPClassifier((10,), activation='logistic')\n",
    "model3_2 = MLPClassifier((10,), activation='logistic')\n",
    "\n",
    "model1_1.fit(X_train_1.todense(), Y_train_1)\n",
    "model1_2.fit(X_train_2.todense(), Y_train_2)\n",
    "model2_1.fit(X_train_1, Y_train_1)\n",
    "model2_2.fit(X_train_2, Y_train_2)\n",
    "model3_1.fit(X_train_1, Y_train_1)\n",
    "model3_2.fit(X_train_2, Y_train_2)\n",
    "\n",
    "accuracy1_1 = model1_1.score(X_test_1.todense(), Y_test_1)\n",
    "accuracy1_2 = model1_2.score(X_test_2.todense(), Y_test_2)\n",
    "accuracy2_1 = model2_1.score(X_test_1, Y_test_1)\n",
    "accuracy2_2 = model2_2.score(X_test_2, Y_test_2)\n",
    "accuracy3_1 = model3_1.score(X_test_1, Y_test_1)\n",
    "accuracy3_2 = model3_2.score(X_test_2, Y_test_2)\n",
    "\n",
    "print('Accuracy GaussianNB + CountVectorizer', accuracy1_1)\n",
    "print('Accuracy GaussianNB + Tfidf', accuracy1_2)\n",
    "print('Accuracy LogisticRegression + CountVectorizer', accuracy2_1)\n",
    "print('Accuracy LogisticRegression + Tfidf', accuracy2_2)\n",
    "print('Accuracy MLPClassifier + CountVectorizer', accuracy3_1)\n",
    "print('Accuracy MLPClassifier + Tfidf', accuracy3_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Atividade: Crie um classificador para diferenciar pessoas fisicas ou juridicas</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM PEOPLE 12000\n",
      "NUM COMPANIES 13085\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "corpus = pickle.load(open('datasets/name_company_corpus.pickle', 'rb'))\n",
    "\n",
    "names = [x[0] for x in corpus if x[1] == 'NAME']\n",
    "companies = [x[0] for x in corpus if x[1] == 'COMPANY']\n",
    "\n",
    "all_texts = names + companies\n",
    "labels = [0]*(len(names)) + [1]*(len(companies))\n",
    "print('NUM PEOPLE', len(names))\n",
    "print('NUM COMPANIES', len(companies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
